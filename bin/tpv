#!/usr/bin/python
# coding: utf-8

### MODULE: common.date

from time import time as get_time
from time import sleep
import time
import datetime

def timedelta( delta ):
    return TimeDelta(
        seconds = delta.seconds,
        days    = delta.days,
    )

class TimeDelta( datetime.timedelta ):
    def total_seconds(self):
        return self.seconds + self.days*24*60*60

def seconds(*args, **nargs):
    d = datetime.timedelta(*args, **nargs)
    return d.seconds + d.days*24*60*60

def date2timestamp(date):
    return time.mktime( date.timetuple() )

def secfmt(seconds):
    int_secs = int(seconds)
    msecs = int((seconds - int_secs)*1000000)
    secs  = int_secs % 60
    mins  = int_secs//60 % 60
    hours = int_secs//60//60 % 24
    days  = int_secs//60//60//24
    if days:
        return "%sd,%02d:%02d:%02d" % (days, hours, mins, secs)
    else:
        return "%02d:%02d:%02d" % (hours, mins, secs)

class StopWatch(object):

    __slots__ = ('value',)

    def __init__(self, value=None):
        self.value = value or get_time()

    def get(self):
        return get_time() - self.value

    def get_and_reset(self):
        new_time = get_time()
        diff = self.value - new_time
        self.value = new_time
        return diff

    def sleep(self, timeout, tick=None):
        """
        Sleep 'timeout' seconds if time elapsed from start is less
        than 'timeout'.
        
        If 'tick' is specified then sleep will be at most 'tick' seconds
        long, this is to use inside loop when external condition should
        be considered to abort sleeping, return value is boolean telling
        whether we need to sleep more, e.g.

          while not process_killed and stop_watch.sleep(timeout=10, tick=1):
              pass

        """
        diff = self.get()
        if diff < timeout:
            pause = timeout-diff
            if tick != None and tick < pause:
                pause = tick
            if pause > 0:
                sleep( pause )
                return True
            else:
                return False

class Timer(object):

    def __init__(self, duration):
        self.duration = duration
        self.sw       = StopWatch()

    def time_left(self, max_time=None):
        if max_time == None:
            max_time = self.duration
        time_left = self.duration - self.sw.get()
        if time_left > max_time:
            time_left = max_time
        if time_left <= 0:
            time_left = 0
        return time_left



### MODULE: common.colls

# coding: utf-8

import threading
import weakref
import itertools
import copy
try:
    import hashlib
    md5_func = hashlib.md5
except ImportError:
    import md5
    md5_func = md5.new


class CustomObject(object):
    """
    >>> CustomObject(a=1, b=2, c='c').__dict__
    {'a': 1, 'c': 'c', 'b': 2}
    """
    def __init__(self, **nargs):
        for attr, value in nargs.iteritems():
            setattr( self, attr, value )
    def __repr__(self):
        return "%s(**%r)" % (self.__class__.__name__, self.__dict__)

def half_md5(string):
    """
    >>> half_md5('hello world!')
    9787753280732890743L
    """
    return int( md5_func( string ).hexdigest()[16:], 16 )

class PreviewIter:
    """
    >>> pi = PreviewIter([1, 2, 3, 4])
    >>> pi.preview()
    1
    >>> pi.is_empty()
    False
    >>> list(pi)
    [1, 2, 3, 4]
    >>> pi.is_empty()
    True
    """

    def __init__( self, iterable ):
        self.iter = iter( iterable )

    def __iter__(self):
        return self

    def __nonzero__(self):
        return not self.is_empty()

    def next( self ):
        if hasattr( self, '_preview' ):
            item = self._preview
            del self._preview
            return item
        else:
            return self.iter.next()

    def preview( self ):
        if not hasattr( self, '_preview' ):
            self._preview = self.iter.next()
        return self._preview

    def is_empty(self):
        try:
            self.preview()
            return False
        except StopIteration:
            return True

def IterChunkIter( iterable, chunk_size ):
    """
    >>> map(list, IterChunkIter( [1,2,3,4,5,6,7,8], 3 ))
    [[1, 2, 3], [4, 5, 6], [7, 8]]
    """
    iterator = iter( iterable )
    while 1:
        chunk = PreviewIter( itertools.islice( iterator, chunk_size ) )
        if chunk.is_empty():
            break
        yield chunk

def ChunkIter( iterable, chunk_size ):
    """
    >>> list( ChunkIter( [1,2,3,4,5,6,7,8], 3 ) )
    [[1, 2, 3], [4, 5, 6], [7, 8]]
    """
    iterator = iter( iterable )
    while 1:
        chunk = list( itertools.islice( iterator, chunk_size ) )
        if not len( chunk ): break
        yield chunk

class IterWithLength:
    """
    >>> it = (i for i in xrange(5))
    >>> l = IterWithLength(it, 5)
    >>> len(l)
    5
    >>> list(l)
    [0, 1, 2, 3, 4]
    """
    def __init__(self, iterable, len):
        self.iterable = iterable
        self.len = len
        
    def __len__(self,):
        return self.len
    
    def __iter__(self,):
        return iter(self.iterable)

def len_iter(iterable):
    len = 0
    for i in iterable:
        len += 1
    return len

class DefaultDict(dict):
    """
    >>> d = DefaultDict(set)
    >>> d[1].update([1, 2, 3])
    >>> d[1].update([2, 3, 4])
    >>> d
    {1: set([1, 2, 3, 4])}
    >>> d[2]
    set([])
    >>> d
    {1: set([1, 2, 3, 4]), 2: set([])}
    """

    def __init__(self, item_factory, items=None):
        super(DefaultDict, self).__init__(items or [])
        self.item_factory = item_factory

    def __getitem__(self, key):
        try:
            item = super(DefaultDict, self).__getitem__(key)
        except KeyError:
            item = self.item_factory()
            self[key] = item
        return item

class Counter(object):
    """
    >>> counter = Counter()
    >>> int(counter)
    0
    >>> items = counter([1, 2, 3, 4])
    >>> items.__class__.__name__
    'generator'
    >>> list(items)
    [1, 2, 3, 4]
    >>> int(counter)
    4
    """

    def __init__(self):
        self.cnt = 0
    def __int__(self):
        return self.cnt
    def __str__(self):
        return str(self.cnt)
    def get_and_reset(self):
        cnt = self.cnt
        self.cnt = 0
        return cnt
    def __call__(self, items):
        for item in items:
            self.cnt += 1
            yield item

class Singleton(object):
    """ A Pythonic Singleton """
    def __new__(cls, *args, **kwargs):
        if '_inst' not in vars(cls):
            cls._inst = object.__new__(cls, *args, **kwargs)
        return cls._inst

def Memoizer(func, args=[], kwargs={}):
    memo = [None, False]
    lock = threading.Lock()
    def memoized():
        lock.acquire()
        try:
            if not memo[1]:
                memo[0] = func(*args, **kwargs)
                memo[1] = True
        finally:
            lock.release()
        return memo[0]
    return memoized

class RoundRobinArray:
    """
    >>> rra = RoundRobinArray(3)
    >>> rra.put(1)
    >>> list(rra)
    [1]
    >>> rra.put(2)
    >>> rra.put(3)
    >>> list(rra)
    [1, 2, 3]
    >>> rra.put(4)
    >>> list(rra)
    [2, 3, 4]
    """

    def __init__( self, size ):
        self.array   = []
        self.end_ptr = 0
        self.size    = size

    def put( self, item ):
        if len( self.array ) < self.size:
            self.array.append( item )
        else:
            self.array[ self.end_ptr ] = item
        self.end_ptr = ( self.end_ptr + 1 ) % self.size

    def __iter__(self):
        return iter( self.array[self.end_ptr:] + self.array[:self.end_ptr] )

def progress_bar(iterator, precision=None, total=None, period=1):
    """ Shortcut for using ProgressBar """
    return ProgressBar(precision, period)(iterator, total)

import sys, time
pass # from common.date import secfmt

def ProgressBar(precision=None, period=1):
    """ Useful to monitor long running console processes """

    precision = precision
    period = period
    gettime = time.time

    def ProgressBarInstance(iterable, total=None):
        speed = 0
        cnt = 0
        started = gettime()
        last_report_time = 0
        last_report_cnt = 0
        if total == None and hasattr(iterable, '__len__'):
            total = len(iterable)

        def time_to_log():
            if precision == None:
                if gettime() - last_report_time >= period:
                    return True
            else:
                if cnt % precision == 0:
                    return True
            return False

        for item in iterable:
            yield item
            cnt += 1

            if time_to_log():
                curtime = gettime()
                speed = (cnt + 1) / (curtime - started)
                mspeed = (cnt - last_report_cnt) / (curtime - last_report_time)
                msg = "%8d (%0.3f ips, %0.3f current)" % (cnt, speed, mspeed)
                if total and speed:
                    msg += " %0.1f%% (left %s of %s)" % (
                        100 * float(cnt + 1) / total,
                        secfmt((total - cnt) / speed),
                        secfmt(total / speed),
                    )
                print >> sys.stderr, "progress_bar:", msg
                last_report_time = curtime
                last_report_cnt = cnt

        speed = (cnt + 1) / (gettime() - started)
        print >> sys.stderr, "progress_bar: summary %d in %s at %0.3f ips" % (
            cnt + 1,
            secfmt(gettime() - started),
            speed,
        )

    return ProgressBarInstance

class SummableSet(set):
    """
    >>> SummableSet([1, 2, 3]) + SummableSet([2, 3, 4])
    SummableSet([1, 2, 3, 4])
    >>> s = SummableSet([1, 2, 3])
    >>> s += SummableSet([2, 3, 4])
    >>> s
    SummableSet([1, 2, 3, 4])
    """

    def __iadd__(self, set2):
        self.update(set2)
        return self

    def __add__(self, set2):
        return self.union(iter(set2))

    __radd__ = __add__

def accum(pairs):
    return Accum.from_pairs(pairs)

class Accum(dict):
    """
    >>> Accum({1:2, 2:3})
    {1: 2, 2: 3}

    >>> a = Accum([1, 2, 1, 2, 3])
    >>> a
    {1: 2, 2: 2, 3: 1}
    >>> a.accum({3:9, 2:18})
    >>> a.accum([3, 2, 2])
    >>> a
    {1: 2, 2: 22, 3: 11}
    >>> a.total()
    35
    >>> for key, cnt, rel in a.iternormal():
    ...     print "%s %3s %0.3f" % (key, cnt, rel)
    1   2 0.057
    2  22 0.629
    3  11 0.314

    >>> a = Accum.from_pairs((
    ...     ('a', Accum([1,1,2,2])     ),
    ...     ('s', SummableSet([1,2,3]) ),
    ...     ('a', Accum([1,2,2,3])     ),
    ...     ('s', SummableSet([2,3,4]) ),
    ... ))
    >>> a
    {'a': {1: 3, 2: 4, 3: 1}, 's': SummableSet([1, 2, 3, 4])}
    """

    def __init__(self, values=None):
        super(Accum, self).__init__()
        if values:
            self.accum(values)

    @classmethod
    def from_pairs(cls, values):
        self = cls()
        for value, cnt in values:
            self.add(value, cnt)
        return self

    def add(self, value, cnt=1):
        try:
            self[value] += cnt
        except KeyError:
            self[value] = cnt

    def accum(self, values):
        self += values

    def accum_pairs(self, values):
        for value, cnt in values:
            self.add(value, cnt)

    def __iadd__(self, values):
        if isinstance(values, dict):
            for value, cnt in values.iteritems():
                try:
                    self[value] += cnt
                except KeyError:
                    self[value] = cnt
        else:
            for val in values:
                self[val] = self.get(val, 0) + 1
        return self

    def __add__(self, stat2):
        stat = Accum()
        if self != 0:
            stat += self
        if stat2 != 0:
            stat += stat2
        return stat
    __radd__ = __add__

    def total(self):
        return sum(self.itervalues())

    def iternormal(self):
        total = 0.0
        if self:
            total = float(sum(self.itervalues()))
        for key, val in self.iteritems():
            yield key, val, val/total

class StatObject(CustomObject):
    """
    >>> s1 = StatObject(a=1,b=2)
    >>> s2 = StatObject(a=3,b=4,c=5,d=Accum([1,1,2,2,3,3,3]))
    >>> s3 = s1+s2

    >>> s3.__dict__
    {'a': 4, 'c': 5, 'b': 6, 'd': {1: 2, 2: 2, 3: 3}}

    >>> s1.__dict__
    {'a': 1, 'b': 2}

    >>> s2.__dict__
    {'a': 3, 'c': 5, 'b': 4, 'd': {1: 2, 2: 2, 3: 3}}

    >>> s2 += s3
    >>> s2.__dict__
    {'a': 7, 'c': 10, 'b': 10, 'd': {1: 4, 2: 4, 3: 6}}
    """

    def __add__(self, stat2):
        stat = StatObject()
        if self != 0:
            stat += self
        if stat2 != 0:
            stat += stat2
        return stat
    __radd__ = __add__

    def __iadd__(self, data):
        if not isinstance(data, StatObject):
            class_name = self.__class__.__name__
            raise TypeError(
                'can only add %s (not "%s") to %s' % (
                    class_name,
                    data.__class__.__name__,
                    class_name,
                )
            )
        for key, val in data.__dict__.iteritems():
            try:
                curr_val = getattr(self, key)
            except AttributeError:
                setattr(self, key, copy.deepcopy(val))
            else:
                setattr(self, key, curr_val + val)
        return self

class _Sentinel(object):
    pass

def layer_merge_ordered(iterables, null=None, order_func=None):
    """
    Склеивает отсортированные последовательности.
    Сохраняя информацию о том, какой элемент из какой последовательности
    пришел.

    >>> list(layer_merge_ordered((
    ...     [1, 3, 5],
    ...     [2, 4, 6],
    ...     [3, 4, 5],
    ... ), null=0))
    [[1, 0, 0], [0, 2, 0], [3, 0, 3], [0, 4, 4], [5, 0, 5], [0, 6, 0]]
    """

    sentinel = _Sentinel()
    def next_if_any(iterator):
        try:
            return iterator.next()
        except StopIteration:
            return sentinel

    order_func = order_func or ( lambda item: item )
    iterators  = [ iter(iterable) for iterable in iterables ]
    items      = [ next_if_any(iterator) for iterator in iterators ]
    while sum( int(item != sentinel) for item in items ):
        min_order = min(order_func(item) for item in items if item != sentinel)
        layer     = []
        for i, item in enumerate( items ):
            if item != sentinel and order_func(item) == min_order:
                layer.append( item )
                items[i] = next_if_any( iterators[i] )
            else:
                layer.append( null )
        yield layer

def merge_ordered(iterables, key=None):
    """
    Склеивает отсортированные последовательности.

    >>> list(merge_ordered((
    ...     [1, 3],
    ...     [2, 4, 6],
    ...     [3, 4, 5, 6, 7],
    ... )))
    [1, 2, 3, 3, 4, 4, 5, 6, 6, 7]
    """

    key = key or ( lambda item: item )
    iters = []
    for iterable in iterables:
        iterator = PreviewIter(iterable)
        try:
            iterator.preview()
            iters.append(iterator)
        except StopIteration:
            pass
    iters.sort(key=lambda iterator: key(iterator.preview()))
    iters_len = len(iters)

    while iters_len > 1:
        yield iters[0].next()
        try:
            new_item = iters[0].preview()
            new_item_key = key(new_item)
            if new_item_key > key(iters[1].preview()):
                free_iterator = iters.pop(0)
                for iter_num, iterator in enumerate(iters):
                    if new_item_key < key(iterator.preview()):
                        iters.insert(iter_num, free_iterator)
                        free_iterator = None
                        break
                if free_iterator:
                    iters.append(free_iterator)
        except StopIteration:
            del iters[0]
            iters_len -= 1

    if iters_len:
        for item in iters[0]:
            yield item

class List(object):

    @classmethod
    def merge_ordered(cls, lists, null=None, order_func=None):
        return layer_merge_ordered(lists, null, order_func)

    @classmethod
    def diff_ordered( cls, list1, list2, attr=None ):
        added   = []
        removed = []

        idx1 = 0;
        idx2 = 0;

        while idx1 < len( list1 ) or idx2 < len( list2 ):
            if  idx2 >= len( list2 ):
                removed.append( list1[idx1] )
                idx1 += 1
            elif idx1 >= len( list1 ):
                added.append( list2[idx2] )
                idx2 += 1
            elif cls._cmp( list1[idx1], list2[idx2], attr ) == -1:
                removed.append( list1[idx1] )
                idx1 += 1
            elif cls._cmp( list1[idx1], list2[idx2], attr ) == 1:
                added.append( list2[idx2] )
                idx2 += 1
            else:
                idx1 += 1
                idx2 += 1

        return added, removed

    @staticmethod
    def _cmp( obj1, obj2, attr ):
        if attr:
            return cmp( getattr( obj1, attr ), getattr( obj2, attr ) )
        else:
            return cmp( obj1, obj2 )

class ObjCache:

    def __init__( self, size, keys ):
        self.lock  = threading.Lock()
        self.items = RoundRobinArray( size )
        self.dicts = {}
        for key in keys:
            self.dicts[ key ] = weakref.WeakValueDictionary()

    def get( self, key, value ):
        item = None
        self.lock.acquire()
        try:
            item = self.dicts[ key ][ value ]
        except KeyError:
            pass
        self.lock.release()
        return item

    def add( self, item ):
        self.lock.acquire()
        try:
            self.items.put( item )
            for key, dct in self.dicts.iteritems():
                dct[ getattr( item, key ) ] = item
        finally:
            self.lock.release()

    def has_key( self, key ):
        self.lock.acquire()
        res = self.dicts.has_key( key )
        self.lock.release()
        return res

    def cached_objects( self, key=None ):
        self.lock.acquire()
        if not key:
            key = self.dicts.iterkeys().next()
        objects = self.dicts[ key ].values()
        self.lock.release()
        return objects

    def keys( self ):
        self.lock.acquire()
        keys = self.dicts.keys()
        self.lock.release()
        return keys

    def reset( self ):
        self.lock.acquire()
        self.items = RoundRobinArray( self.items.size )
        for key in self.dicts.keys():
            self.dicts[ key ] = weakref.WeakValueDictionary()
        self.lock.release()

### MODULE: common.namedtuple

try:
    from collections import namedtuple
except ImportError:
    from operator import itemgetter as _itemgetter
    from keyword import iskeyword as _iskeyword
    import sys as _sys

    def namedtuple(typename, field_names, verbose=False, rename=False):
        """Returns a new subclass of tuple with named fields.

        >>> Point = namedtuple('Point', 'x y')
        >>> Point.__doc__                   # docstring for the new class
        'Point(x, y)'
        >>> p = Point(11, y=22)             # instantiate with positional args or keywords
        >>> p[0] + p[1]                     # indexable like a plain tuple
        33
        >>> x, y = p                        # unpack like a regular tuple
        >>> x, y
        (11, 22)
        >>> p.x + p.y                       # fields also accessable by name
        33
        >>> d = p._asdict()                 # convert to a dictionary
        >>> d['x']
        11
        >>> Point(**d)                      # convert from a dictionary
        Point(x=11, y=22)
        >>> p._replace(x=100)               # _replace() is like str.replace() but targets named fields
        Point(x=100, y=22)

        """

        # Parse and validate the field names.  Validation serves two purposes,
        # generating informative error messages and preventing template injection attacks.
        if isinstance(field_names, basestring):
            field_names = field_names.replace(',', ' ').split() # names separated by whitespace and/or commas
        field_names = tuple(map(str, field_names))
        if rename:
            names = list(field_names)
            seen = set()
            for i, name in enumerate(names):
                if (not min(c.isalnum() or c=='_' for c in name) or _iskeyword(name)
                    or not name or name[0].isdigit() or name.startswith('_')
                    or name in seen):
                        names[i] = '_%d' % i
                seen.add(name)
            field_names = tuple(names)
        for name in (typename,) + field_names:
            if not min(c.isalnum() or c=='_' for c in name):
                raise ValueError('Type names and field names can only contain alphanumeric characters and underscores: %r' % name)
            if _iskeyword(name):
                raise ValueError('Type names and field names cannot be a keyword: %r' % name)
            if name[0].isdigit():
                raise ValueError('Type names and field names cannot start with a number: %r' % name)
        seen_names = set()
        for name in field_names:
            if name.startswith('_') and not rename:
                raise ValueError('Field names cannot start with an underscore: %r' % name)
            if name in seen_names:
                raise ValueError('Encountered duplicate field name: %r' % name)
            seen_names.add(name)

        # Create and fill-in the class template
        numfields = len(field_names)
        argtxt = repr(field_names).replace("'", "")[1:-1]   # tuple repr without parens or quotes
        reprtxt = ', '.join('%s=%%r' % name for name in field_names)
        template = '''class %(typename)s(tuple):
        '%(typename)s(%(argtxt)s)' \n
        __slots__ = () \n
        _fields = %(field_names)r \n
        def __new__(_cls, %(argtxt)s):
            return _tuple.__new__(_cls, (%(argtxt)s)) \n
        @classmethod
        def _make(cls, iterable, new=tuple.__new__, len=len):
            'Make a new %(typename)s object from a sequence or iterable'
            result = new(cls, iterable)
            if len(result) != %(numfields)d:
                raise TypeError('Expected %(numfields)d arguments, got %%d' %% len(result))
            return result \n
        def __repr__(self):
            return '%(typename)s(%(reprtxt)s)' %% self \n
        def _asdict(self):
            'Return a new dict which maps field names to their values'
            return dict(zip(self._fields, self)) \n
        def _replace(_self, **kwds):
            'Return a new %(typename)s object replacing specified fields with new values'
            result = _self._make(map(kwds.pop, %(field_names)r, _self))
            if kwds:
                raise ValueError('Got unexpected field names: %%r' %% kwds.keys())
            return result \n
        def __getnewargs__(self):
            return tuple(self) \n\n''' % locals()
        for i, name in enumerate(field_names):
            template += '        %s = _property(_itemgetter(%d))\n' % (name, i)
        if verbose:
            print template

        # Execute the template string in a temporary namespace
        namespace = dict(_itemgetter=_itemgetter, __name__='namedtuple_%s' % typename,
                         _property=property, _tuple=tuple)
        try:
            exec template in namespace
        except SyntaxError, e:
            raise SyntaxError(e.message + ':\n' + template)
        result = namespace[typename]

        # For pickling to work, the __module__ variable needs to be set to the frame
        # where the named tuple is created.  Bypass this step in enviroments where
        # sys._getframe is not defined (Jython for example) or sys._getframe is not
        # defined for arguments greater than 0 (IronPython).
        try:
            result.__module__ = _sys._getframe(1).f_globals.get('__name__', '__main__')
        except (AttributeError, ValueError):
            pass

        return result

# if __name__ == '__main__':
#     # verify that instances can be pickled
#     from cPickle import loads, dumps
#     Point = namedtuple('Point', 'x, y', True)
#     p = Point(x=10, y=20)
#     assert p == loads(dumps(p, -1))
# 
#     # test and demonstrate ability to override methods
#     class Point(namedtuple('Point', 'x y')):
#         @property
#         def hypot(self):
#             return (self.x ** 2 + self.y ** 2) ** 0.5
#         def __str__(self):
#             return 'Point: x=%6.3f y=%6.3f hypot=%6.3f' % (self.x, self.y, self.hypot)
# 
#     for p in Point(3,4), Point(14,5), Point(9./7,6):
#         print p
# 
#     class Point(namedtuple('Point', 'x y')):
#         'Point class with optimized _make() and _replace() without error-checking'
#         _make = classmethod(tuple.__new__)
#         def _replace(self, _map=map, **kwds):
#             return self._make(_map(kwds.get, ('x', 'y'), self))
# 
#     print Point(11, 22)._replace(x=100)
# 
#     import doctest
#     TestResults = namedtuple('TestResults', 'failed attempted')
#     print TestResults(*doctest.testmod())

### MODULE: tabkit.datasrc

import os
from itertools import izip
pass # from common.namedtuple import namedtuple

TYPES = set([
    'float',
    'int',
    'str',
    'bool',
    'any',
])

class DataField(namedtuple('DataField', 'name type')):
    def __repr__(self):
        return "%s(%r, %r)" % (
            self.__class__.__name__, self.name, self.type
        )

class DataFieldOrder(object):
    def __init__(self, name, desc=None, numeric=None):
        self.name = name
        self.desc = desc
        self.numeric = numeric
        if self.desc == None:
            self.desc = False
        if self.numeric == None:
            self.numeric = False

    def __eq__(self, other):
        return (
            self.name == other.name
            and self.desc == other.desc
            and self.numeric == other.numeric
        )

    def __ne__(self, other):
        return not (self == other)

    def __repr__(self):
        return "%s(%r, desc=%r, numeric=%r)" % (
            self.__class__.__name__, self.name, self.desc, self.numeric
        )

class DataOrder(object):
    def __init__(self, data_order):
        self.data_order = data_order

    def fields_are_ordered(self, field_names):
        for field_name, order in zip(field_names, self.data_order):
            if field_name != order.name:
                return False
        return True

    def is_ordered_by(self, data_order):
        if isinstance(data_order, DataOrder):
            data_order = data_order.data_order
        data_order = list(data_order)
        if len(self.data_order) < len(data_order):
            return False
        for required, actual in zip(data_order, self.data_order):
            if required != actual:
                return False
        return True

    def __nonzero__(self):
        return bool(self.data_order)

    def __iter__(self):
        return iter(self.data_order)

    def __repr__(self):
        return "%s(%r)" % (
            self.__class__.__name__, self.data_order
        )

class DataDesc(object):
    def __init__(self, fields, order=None, size=None):
        self.size = size
        self.fields = fields
        if isinstance(order, DataOrder):
            self.order = order
        else:
            self.order = DataOrder(order or [])
        self.field_names = dict((field.name, idx) for idx, field in enumerate(self.fields))
        if len(self.fields) != len(self.field_names):
            raise Exception("Conflicting field names in %r" % (self.fields,))
        for order_field in self.order:
            if order_field.name not in self.field_names:
                raise Exception('Unknown ordering field name %r' % (order_field.name,))

    def field_index(self, name):
        return self.field_names[name]

    def get_field(self, name):
        return self.fields[self.field_names[name]]

    def has_field(self, name):
        return name in self.field_names

    def __repr__(self):
        return "%s(%r, %r)" % (
            self.__class__.__name__, self.fields, self.order
        )

def parse_data(fields, lines):
    Row = namedtuple('Row', tuple(field.name for field in fields))
    for line in lines:
        values = line.rstrip('\r\n').split('\t')
        yield Row(*tuple(field.type(val) for val, field in izip(values, fields)))

class DataSrc(object):
    def __init__(self, iterator, data_desc):
        self.iterator = iterator
        self.data_desc = data_desc
    def __iter__(self):
        return self.iterator

class DataSrcShell(object):
    def __init__(self, cmd, data_desc):
        self.cmd = cmd
        self.data_desc = data_desc
    def __iter__(self):
        return parse_data(self.data_desc.fields, os.popen(self.cmd))
    def get_fname(self):
        return '<(%s)' % (self.cmd,)

def merge_data_desc(desc1, desc2):
    return DataDesc(
        fields = merge_data_fields(desc1.fields, desc2.fields),
        order = merge_data_order(desc1.order, desc2.order),
    )

def merge_data_fields(fields1, fields2):
    if len(fields1) != len(fields2):
        raise Exception('Incompatible data fields: %r and %r' % (
            fields1, fields2,
        ))

    fields = []
    for field1, field2 in zip(fields1, fields2):
        if field1.name != field2.name:
            raise Exception('Incompatible data fields: %r and %r' % (
                fields1, fields2,
            ))
        if field1.type == field2.type:
            fields.append(DataField(field1.name, field1.type))
        elif field1.type == 'any':
            fields.append(DataField(field1.name, field2.type))
        elif field2.type == 'any':
            fields.append(DataField(field1.name, field1.type))
        else:
            raise Exception('Incompatible data fields: %r and %r' % (
                fields1, fields2,
            ))

    return fields

def merge_data_order(order1, order2):
    order = []
    for ord1, ord2 in zip(order1, order2):
        if ord1 == ord2:
            order.append(DataFieldOrder(ord1.name, ord1.desc, ord1.numeric))
        else:
            break
    return DataOrder(order)

def rename_fields(desc, renamings):
    new_fields = []
    for field in desc.fields:
        if field.name in renamings:
            new_fields.append(DataField(renamings[field.name], field.type))
        else:
            new_fields.append(DataField(field.name, field.type))

    new_order = []
    for field in desc.order:
        if field.name in renamings:
            new_order.append(DataFieldOrder(
                renamings[field.name], desc=field.desc, numeric=field.numeric
            ))
        else:
            new_order.append(DataFieldOrder(
                field.name, desc=field.desc, numeric=field.numeric
            ))

    return DataDesc(new_fields, new_order)


### MODULE: tabkit.header

import os
import re

pass # from tabkit.datasrc import DataField, DataFieldOrder, DataDesc

def field_split(fields_str):
    return re.split(r'[;,\s]+', fields_str.strip())

def parse_header_order(zone_fields):
    for order_field_str in zone_fields:
        order_field = order_field_str.strip().split(':')
        field_name = order_field[0]
        desc = None
        numeric = None
        for modifier in order_field[1:]:
            if modifier in ('asc', 'desc'):
                if desc != None:
                    raise Exception('Ambiguous order direction in %r' % (order_field_str,))
                if modifier == 'asc':
                    desc = False
                else:
                    desc = True
            elif modifier == 'num':
                numeric = True
        yield DataFieldOrder(field_name, desc, numeric)

def parse_header(header):
    """
    >>> header = "# shows:int clicks:int ctr:float rel url # order: url:asc, ctr:desc:num"
    >>> parse_header(header) #doctest: +NORMALIZE_WHITESPACE
    DataDesc([DataField('shows', 'int'),
            DataField('clicks', 'int'),
            DataField('ctr', 'float'),
            DataField('rel', 'any'),
            DataField('url', 'any')],
        DataOrder([DataFieldOrder('url', desc=False, numeric=False),
            DataFieldOrder('ctr', desc=True, numeric=True)]))
    """
    if not header.startswith('#'):
        raise Exception('Bad header')
    else:
        data_fields = []
        data_order = []
        data_size = None

        zones = header[1:].split('#')
        fields_str = zones[0]
        fields = field_split(fields_str)
        for field in fields:
            field_parts = field.split(':')
            field_name = field_parts[0]
            field_type = 'any'
            if len(field_parts) == 2:
                field_type = field_parts[1]
            elif len(field_parts) > 2:
                raise Exception('Invalid field %r' % (field,))
            data_fields.append(DataField(field_name, field_type))

        for zone in zones[1:]:
            zone_name, zone_data = zone.strip().split(None, 1)
            if zone_name == 'ORDER:':
                data_order = list(parse_header_order(field_split(zone_data)))
            elif zone_name == 'SIZE:':
                data_size = int(zone_data)
            else:
                raise Exception('Bad header, invalid zone %r' % (zone_name,))

        return DataDesc(data_fields, data_order, data_size)

def make_header(data_desc):
    """
    >>> desc = DataDesc(
    ...     [DataField('shows', 'int'), DataField('url', 'any')],
    ...     [DataFieldOrder('url', desc=True), DataFieldOrder('ctr', numeric=True)],
    ... )
    >>> make_header(desc)
    '# shows:int url #order: url:desc ctr:num\n'
    """
    fields = []
    for field in data_desc.fields:
        if field.type == 'any':
            fields.append(field.name)
        else:
            fields.append(field.name + ':' + field.type)
    order = []
    for order_field in data_desc.order:
        order_str = order_field.name
        if order_field.desc:
            order_str += ':desc'
        if order_field.numeric:
            order_str += ':num'
        order.append(order_str)
    header = '# ' + '\t'.join(fields)
    if order:
        header += ' #ORDER: ' + '\t'.join(order)
    if data_desc.size != None:
        header += ' #SIZE: ' + str(data_desc.size)
    header += "\n"
    return header

def read_fd_header(fd):
    header = ''
    while header[-1:] != '\n':
        ch = os.read(fd, 1)
        if ch == '':
            break
        header += ch
    return header

def read_file_header(fname):
    fobj = open(fname)
    header = fobj.readline()
    fobj.close()
    return header

def _test():
    import doctest
    doctest.testmod()

# if __name__ == "__main__":
#     _test()

### MODULE: tabkit.utils

import sys
import os
from subprocess import Popen, PIPE
from collections import defaultdict
from pipes import quote
from itertools import islice

pass # from tabkit.header import parse_header, read_fd_header, read_file_header
pass # from tabkit.datasrc import DataDesc, merge_data_fields

try:
    from functools import partial
except ImportError:
    def partial(func, *args, **keywords):
        def newfunc(*fargs, **fkeywords):
            newkeywords = keywords.copy()
            newkeywords.update(fkeywords)
            return func(*(args + fargs), **newkeywords)
        newfunc.func = func
        newfunc.args = args
        newfunc.keywords = keywords
        return newfunc

def exception_handler(func):
    if '--pytrace' in sys.argv:
        func()
    else:
        try:
            func()
        except Exception, err:
            print >> sys.stderr, sys.argv[0] + ":", type(err).__name__ + ":", err
            sys.exit(1)

def safe_popen_args(command):
    return ['/bin/bash', '-o', 'pipefail', '-o', 'errexit', '-c', command]

def safe_popen(command, bufsize=None):
    """
    >>> list(safe_popen('echo ok'))
    ['ok\\n']

    >>> list(safe_popen('false; echo ok'))
    Traceback (most recent call last):
        ...
    Exception: safe_popen failed on 'false; echo ok', status = 1

    >>> list(safe_popen('false|true; echo ok'))
    Traceback (most recent call last):
        ...
    Exception: safe_popen failed on 'false|true; echo ok', status = 1
    """
    popen_args = dict(
        args = safe_popen_args(command),
        shell = False,
        stdout = PIPE,
    )
    if bufsize != None:
        popen_args['bufsize'] = bufsize

    popen = Popen(**popen_args)
    for line in popen.stdout:
        yield line
    status = popen.wait()
    if status != 0:
        raise Exception("safe_popen failed on %r, status = %r" % (command, status))

def safe_system(command):
    popen = Popen(args=safe_popen_args(command), shell=False)
    status = popen.wait()
    if status != 0:
        raise Exception("safe_system failed on %r, status = %r" % (command, status))

class FilesList(object):
    def __init__(self, fnames, stdin_fallback=True):
        self.fnames = fnames
        if stdin_fallback and not fnames:
            self.fnames = ['-']
        self.fobjs = []
        self.headers = []
        got_stdin = False
        for fname in self.fnames:
            if fname == '-':
                if got_stdin:
                    raise Exception('"-" specified as input file more than once')
                else:
                    self.headers.append(read_fd_header(sys.stdin.fileno()))
                    self.fobjs.append(sys.stdin)
                    got_stdin = True
            elif os.path.isfile(fname):
                self.headers.append(read_file_header(fname))
                self.fobjs.append(None)
            elif os.path.exists(fname):
                fobj = open(fname)
                self.headers.append(read_fd_header(fobj.fileno()))
                self.fobjs.append(fobj)
            else:
                raise Exception('File does not exist: %r' % (fname,))

    def __len__(self):
        return len(self.fnames)

    def get_size(self):
        size = 0
        for fname, desc in self.names_descs():
            if os.path.isfile(fname):
                size += os.stat(fname).st_size
            elif desc.size != None:
                size += desc.size
            else:
                return None
        return size

    def names_descs(self):
        for fname, header in zip(self.fnames, self.headers):
            yield fname, parse_header(header)

    def concat_desc(self):
        fields = None
        for fname, desc in self.names_descs():
            if fields:
                fields = merge_data_fields(fields, desc.fields)
            else:
                fields = desc.fields
        return DataDesc(fields, [])

    def cmd_args(self):
        args = []
        for fname, fobj in zip(self.fnames, self.fobjs):
            args.append(fobj2cmdarg(fobj, fname))
        return args

    def cmd_args_str(self):
        return ' '.join(self.cmd_args())

def fobj2cmdarg(fobj, fname=None):
    if fname == None:
        fname = fobj.name
    if os.path.isfile(fname):
        return "<(tail -qn +2 %s)" % (quote(fname),)
    else:
        return '/dev/fd/%s' % (fobj.fileno(),)

def parse_renamings(renaming_opts):
    renamings = defaultdict(dict)
    for rename in renaming_opts:
        left, right_name = rename.split('=')
        left_file, left_name = left.split('.')
        renamings[int(left_file) - 1][left_name] = right_name
    return renamings

def proper_reduce(func, args):
    args_iter = iter(args)
    res = list(islice(args_iter, 2))
    if len(res) == 1:
        return res[0]
    else:
        res = func(*list(res))
        for arg in args_iter:
            res = func(res, arg)
        return res


### MODULE: tabkit.miniast

from _ast import *
from _ast import __version__

def parse(expr, filename='<unknown>', mode='exec'):
    """
    Parse an expression into an AST node.
    Equivalent to compile(expr, filename, mode, PyCF_ONLY_AST).
    """
    return compile(expr, filename, mode, PyCF_ONLY_AST)

def dump(node, annotate_fields=True, include_attributes=False):
    """
    Return a formatted dump of the tree in *node*.  This is mainly useful for
    debugging purposes.  The returned string will show the names and the values
    for fields.  This makes the code impossible to evaluate, so if evaluation is
    wanted *annotate_fields* must be set to False.  Attributes such as line
    numbers and column offsets are not dumped by default.  If this is wanted,
    *include_attributes* can be set to True.
    """
    def _format(node):
        if isinstance(node, AST):
            fields = [(a, _format(b)) for a, b in iter_fields(node)]
            rv = '%s(%s' % (node.__class__.__name__, ', '.join(
                ('%s=%s' % field for field in fields)
                if annotate_fields else
                (b for a, b in fields)
            ))
            if include_attributes and node._attributes:
                rv += fields and ', ' or ' '
                rv += ', '.join('%s=%s' % (a, _format(getattr(node, a)))
                                for a in node._attributes)
            return rv + ')'
        elif isinstance(node, list):
            return '[%s]' % ', '.join(_format(x) for x in node)
        return repr(node)
    if not isinstance(node, AST):
        raise TypeError('expected AST, got %r' % node.__class__.__name__)
    return _format(node)

def iter_fields(node):
    """
    Yield a tuple of ``(fieldname, value)`` for each field in ``node._fields``
    that is present on *node*.
    """
    if node._fields:
        for field in node._fields:
            try:
                yield field, getattr(node, field)
            except AttributeError:
                pass



### MODULE: tabkit.awk

import _ast

pass # from tabkit.miniast import parse, dump
pass # from tabkit.datasrc import DataDesc, DataField

OP_MAP = {
    _ast.Add      : lambda args: RowExprOp('+', args),
    _ast.Sub      : lambda args: RowExprOp('-', args),
    _ast.Mult     : lambda args: RowExprOp('*', args),
    _ast.Div      : lambda args: RowExprOp('/', args),
    _ast.Pow      : lambda args: RowExprOp('^', args),
    _ast.Mod      : lambda args: RowExprOp('%', args),
    _ast.FloorDiv : lambda args: RowExprFunc('int', [RowExprOp('/', args)]),

    _ast.And      : lambda args: RowExprOp('&&', args),
    _ast.Or       : lambda args: RowExprOp('||', args),
    _ast.Not      : lambda args: RowExprFunc('!', args),

    _ast.Eq      : lambda args: RowExprOp('==', args),
    _ast.NotEq   : lambda args: RowExprOp('!=', args),
    _ast.Gt      : lambda args: RowExprOp('>', args),
    _ast.Lt      : lambda args: RowExprOp('<', args),
    _ast.GtE     : lambda args: RowExprOp('>=', args),
    _ast.LtE     : lambda args: RowExprOp('<=', args),
}

class RowExprAssign(object):
    def __init__(self, target, value):
        self.target = target
        self.value = value
    def tostr(self):
        return self.target + ' = ' + self.value.tostr()

class RowExprConst(object):
    def __init__(self, const):
        self.const = const
        if isinstance(const, str):
            pass
        elif isinstance(const, int):
            pass
        elif isinstance(const, float):
            pass
        elif isinstance(const, bool):
            pass
        else:
            raise Exception("Unsupported type of const %r" % (const,))
    def tostr(self):
        if isinstance(self.const, str):
            return '"' + self.const.replace('"', r'\"') + '"'
        elif isinstance(self.const, bool):
            return str(int(self.const))
        else:
            return str(self.const)
    def find(self, node_type, node_props):
        return match_node(self, node_type, node_props)

class RowExprField(object):
    def __init__(self, ctx, name):
        self.ctx = ctx
        self.name = name
        if self.name.startswith('__'):
            raise Exception("Variable name can't start with '__'")
    def tostr(self):
        return '$' + str(self.ctx.data_desc.field_index(self.name) + 1)
    def find(self, node_type, node_props):
        return match_node(self, node_type, node_props)

class RowExprVar(object):
    def __init__(self, ctx, name):
        self.ctx = ctx
        self.name = name
    def tostr(self):
        if self.ctx.substitute_var(self.name):
            return self.ctx.vars[self.name].tostr()
        else:
            return self.name
    def find(self, node_type, node_props):
        for node in match_node(self, node_type, node_props):
            yield node
        var_expr = self.ctx.vars.get(self.name)
        if var_expr:
            for node in var_expr.find(node_type, node_props):
                yield node

class RowExprFunc(object):
    def __init__(self, func, args):
        self.func = func
        self.args = args
    def tostr(self):
        return self.func + '(' + ', '.join(arg.tostr() for arg in self.args) + ')'
    def find(self, node_type, node_props):
        for node in match_node(self, node_type, node_props):
            yield node
        for arg in self.args:
            for node in arg.find(node_type, node_props):
                yield node

class RowExprOp(object):
    def __init__(self, op, args):
        self.op = op
        self.args = args
    def tostr(self):
        return '(' + (' ' + self.op + ' ').join(arg.tostr() for arg in self.args) + ')'
    def find(self, node_type, node_props):
        for node in match_node(self, node_type, node_props):
            yield node
        for arg in self.args:
            for node in arg.find(node_type, node_props):
                yield node

class ExprContext(object):
    def __init__(self, data_desc, substitute_vars=False):
        self.data_desc = data_desc
        self.vars = {}
        self.varnames = []
        self.substitute_vars = substitute_vars
    def has_field(self, name):
        return self.data_desc.has_field(name)
    def substitute_var(self, name):
        return self.substitute_vars
    def itervars(self):
        for name in self.varnames:
            yield name, self.vars[name]
    def set_var(self, name, val, insert_at=None):
        if name not in self.vars:
            self.vars[name] = val
            if insert_at == None:
                self.varnames.append(name)
            else:
                self.varnames.insert(insert_at, name)
        else:
            raise Exception("Variable named %r already exists" % (name,))

def add_blocks(block1, block2):
    if isinstance(block1, AwkBlock):
        block1 = block1.lines
    elif isinstance(block1, AwkHeadBlock):
        block1 = [block1]
    if isinstance(block2, AwkBlock):
        block2 = block2.lines
    elif isinstance(block2, AwkHeadBlock):
        block2 = [block2]
    return AwkBlock(block1 + block2)

class AwkBlock(object):
    def __init__(self, lines=None):
        self.lines = lines or []
    def __nonzero__(self):
        return bool(self.lines)
    def append(self, line):
        self.lines.append(line)
    def __add__(self, block):
        return add_blocks(self, block)
    def __radd__(self, block):
        return add_blocks(block, self)
    def tostr(self, ident=0, newline='', level=0):
        strs = []
        for line in self.lines:
            if isinstance(line, AwkBlock):
                strs.append(
                    ' '*ident*level + '{' + newline
                    + line.tostr(ident, newline, level+1) + newline
                    + ' '*ident*level + '}'
                )
            elif isinstance(line, AwkHeadBlock):
                strs.append(
                    line.tostr(ident, newline, level)
                )
            else:
                line_str = ' '*ident*level + line
                if not line_str.endswith(';'):
                    line_str += ';'
                strs.append(line_str)
        return newline.join(strs)

class AwkHeadBlock(object):
    def __init__(self, header_str, block):
        self.header_str = header_str
        self.block = block
    def __add__(self, block):
        return add_blocks(self, block)
    def __radd__(self, block):
        return add_blocks(block, self)
    def tostr(self, ident=0, newline='', level=0):
        return (
            ' '*ident*level + self.header_str + newline
            + AwkBlock([self.block]).tostr(ident, newline, level)
        )

class AwkScript(object):
    """
    >>> awk = AwkScript(
    ...     begin = AwkBlock(['a=0']),
    ...     end = AwkBlock(['xx=0']),
    ...     main = AwkBlock([
    ...         'a=1',
    ...         AwkBlock(['b=2', 'c=3', 'd=4; e=5']),
    ...         'print',
    ...     ]),
    ... )
    >>> print awk.tostr(ident=0, newline='')
    BEGIN{a=0;}{a=1;{b=2;c=3;d=4; e=5;}print;}END{xx=0;}
    >>> print awk.tostr(ident=4, newline='\\n')
    BEGIN{
        a=0;
    }
    {
        a=1;
        {
            b=2;
            c=3;
            d=4; e=5;
        }
        print;
    }
    END{
        xx=0;
    }
    <BLANKLINE>
    """
    def __init__(self, main, begin=None, end=None, awk=None):
        self.awk = awk or 'awk'
        self.main = main
        self.begin = begin or AwkBlock([])
        self.end = end or AwkBlock([])
    def tostr(self, ident=0, newline=''):
        awk_str = ''
        if self.begin.lines:
            awk_str += 'BEGIN' + AwkBlock([self.begin]).tostr(ident, newline, 0) + newline
        awk_str += AwkBlock([self.main]).tostr(ident, newline, 0) + newline
        if self.end.lines:
            awk_str += 'END' + AwkBlock([self.end]).tostr(ident, newline, 0) + newline
        return awk_str
    def cmd_line(self):
        return "LC_ALL=C %s -F $'\\t' '%s'" % (self.awk, self.tostr(),)

def parse_expr(ctx, tree, subparser=None):
    subparser = subparser or parse_expr

    if isinstance(tree, _ast.Expr):
        return subparser(ctx, tree.value)
    elif isinstance(tree, _ast.Name):
        if ctx.has_field(tree.id):
            return RowExprField(ctx, tree.id)
        if tree.id in ctx.vars:
            return RowExprVar(ctx, tree.id)
        elif tree.id in ['NR', 'NF']:
            return RowExprVar(ctx, tree.id)
        else:
            raise Exception('Variable %r not found' % (tree.id,))
    elif isinstance(tree, _ast.Call):
        assert not tree.keywords
        return RowExprFunc(
            tree.func.id, list(subparser(ctx, arg) for arg in tree.args)
        )
    elif isinstance(tree, _ast.Num):
        return RowExprConst(tree.n)
    elif isinstance(tree, _ast.Str):
        return RowExprConst(tree.s)
    elif isinstance(tree, _ast.BinOp):
        return OP_MAP[type(tree.op)]([subparser(ctx, tree.left), subparser(ctx, tree.right)])
    elif isinstance(tree, _ast.BoolOp):
        return OP_MAP[type(tree.op)]([subparser(ctx, val) for val in tree.values])
    elif isinstance(tree, _ast.UnaryOp):
        return OP_MAP[type(tree.op)]([subparser(ctx, tree.operand)])
    elif isinstance(tree, _ast.Compare):
        assert len(tree.ops) == 1
        return OP_MAP[type(tree.ops[0])](
            [subparser(ctx, val) for val in [tree.left] + tree.comparators]
        )
    elif isinstance(tree, _ast.Assign):
        raise Exception('Invalid assignment in %r' % (dump(tree),))
    else:
        raise Exception('Unrecognized node %r' % (tree,))

def parse_assign_expr(ctx, tree, subparser):
    if isinstance(tree, _ast.Assign):
        assert len(tree.targets) == 1 and isinstance(tree.targets[0], _ast.Name)
        return RowExprAssign(tree.targets[0].id, subparser(ctx, tree.value))
    else:
        raise Exception("Please assign expression to a variable")

def parse_rowexpr(ctx, tree):
    return parse_assign_expr(ctx, tree, parse_expr)

def match_node(node, node_type, node_props):
    if not isinstance(node, node_type):
        return iter([])
    for name, val in node_props.iteritems():
        if not (hasattr(node, name) and getattr(node, name) == val):
            return iter([])
    return iter([node])

def find_all_refered_vars(ctx):
    refered_vars = set()
    for val in ctx.vars.itervalues():
        for node in val.find(RowExprVar, {}):
            refered_vars.add(node.name)
    return refered_vars

def find_refered_vars(ctx, expr):
    for node in expr.find(RowExprVar, {}):
        yield node

def awk_filter_map(data_desc, filter_str, map_strs):
    """
    >>> from tabkit.header import parse_header
    >>> awk, desc = awk_filter_map(
    ...     parse_header('# d p e s c m'),
    ...     'e==157 and s>100',
    ...     ['ctr=c/s', 'cpm=ctr*m']
    ... )
    >>> print desc
    DataDesc([DataField('ctr', 'any'), DataField('cpm', 'any')], DataOrder([]))
    >>> print awk.cmd_line()
    LC_ALL=C awk -F $'\\t' 'BEGIN{OFS="\\t";}{if((($3 == 157) && ($4 > 100))){ctr = ($5 / $4);print(ctr,(ctr * $6));}}'
    """
    ctx = ExprContext(data_desc)

    # parse map
    for map_expr_str in map_strs:
        for node in parse(map_expr_str).body:
            expr = parse_rowexpr(ctx, node)
            ctx.set_var(expr.target, expr.value)

    # parse filter
    nodes = parse(filter_str).body
    filter_expr = None
    if len(nodes) == 0:
        pass
    elif len(nodes) == 1:
        filter_expr = parse_expr(ctx, nodes[0])
    else:
        raise Exception('Multiple expressions in filter are not allowed')

    awk_cmd, output_desc = awk_filter_map_from_context(ctx, filter_expr)
    return awk_cmd, output_desc or data_desc

def awk_filter_map_from_context(ctx, filter_expr=None):
    refered_by_filter = set()
    if filter_expr != None:
        refered_by_filter = set(node.name for node in find_refered_vars(ctx, filter_expr))
    refered_vars = find_all_refered_vars(ctx)
    assignments0 = []
    assignments1 = []
    statements = []
    for name, val in ctx.itervars():
        if name in refered_by_filter:
            assignments0.append(RowExprAssign(name, val))
            if not name.startswith('_'):
                statements.append(RowExprVar(ctx, name))
        elif name in refered_vars:
            assignments1.append(RowExprAssign(name, val))
            if not name.startswith('_'):
                statements.append(RowExprVar(ctx, name))
        elif not name.startswith('_'):
            statements.append(val)

    awk_cmd = AwkBlock(["print(" + ','.join(expr.tostr() for expr in statements) + ")"])
    if assignments1:
        awk_cmd = AwkBlock(['; '.join(expr.tostr() for expr in assignments1)]) + awk_cmd
    if filter_expr:
        awk_cmd = AwkBlock([AwkHeadBlock('if(%s)' % (filter_expr.tostr(),), awk_cmd)])
    if assignments0:
        awk_cmd = AwkBlock(['; '.join(expr.tostr() for expr in assignments0)]) + awk_cmd
    awk_cmd = AwkScript(awk_cmd, begin=AwkBlock(['OFS="\\t"']))

    # construct data_desc
    output_fields = []
    for name in ctx.varnames:
        if not name.startswith('_'):
            output_fields.append(DataField(name, 'any'))

    if output_fields:
        return awk_cmd, DataDesc(output_fields, [])
    else:
        return awk_cmd, None

def _test():
    import doctest
    doctest.testmod()

# if __name__ == "__main__":
#     _test()
# 

### MODULE: tabkit.awk_grp

import _ast

pass # from tabkit.miniast import parse
pass # from tabkit.utils import partial
pass # from tabkit.datasrc import DataDesc

pass # from tabkit.awk import parse_expr, parse_assign_expr
pass # from tabkit.awk import awk_filter_map_from_context, match_node
pass # from tabkit.awk import ExprContext, RowExprOp, RowExprVar
pass # from tabkit.awk import AwkBlock, AwkScript, AwkHeadBlock

class Namer(object):
    def __init__(self, prefix):
        self.prefix = prefix
        self.cnt = 0
        self.names = {}
    def get_name(self, obj):
        if obj in self.names:
            return self.names[obj]
        else:
            name = self.prefix + str(self.cnt)
            self.names[name] = obj
            self.cnt += 1
            return name

class _GrpExprFunc(RowExprVar):
    def __init__(self, name, init, update, args):
        self.name = name
        self.init = init
        self.update = update
        self.args = args
    def tostr(self):
        return self.name
    def find(self, node_type, node_props):
        return match_node(self, node_type, node_props)
    def _expand_tpl(self, tpl):
        args = dict(var=self.name)
        for num, arg in enumerate(self.args):
            args['rowexpr%s' % (num,)] = arg.tostr()
        return tpl % args
    def init_str(self):
        return self._expand_tpl(self.init)
    def update_str(self):
        return self._expand_tpl(self.update)

class GrpExprFuncMaker(object):
    def __init__(self, prefix):
        self.namer = Namer(prefix)
    def __call__(self, init, update, args):
        return _GrpExprFunc(
            name = self.namer.get_name((init, update) + tuple(arg.tostr() for arg in args)),
            init = init,
            update = update,
            args = args,
        )

def grp_ifmax(maker, args):
    if len(args) != 2:
        raise Exception("'ifmax' function takes 2 arguments")
    return maker(
        init = '%(var)s_cmp = -10^1000000; %(var)s_cmp = -10^1000000;',
        update = (
            '__tmp__=%(rowexpr0)s; '
            'if(__tmp__>%(var)s_cmp)'
            '{%(var)s_cmp=__tmp__; %(var)s=%(rowexpr1)s};'
        ),
        args = args,
    )

def grp_ifmin(maker, args):
    if len(args) != 2:
        raise Exception("'ifmin' function takes 2 arguments")
    return maker(
        init = '%(var)s_cmp = 10^1000000; %(var)s_cmp = 10^1000000;',
        update = (
            '__tmp__=%(rowexpr0)s; '
            'if(__tmp__<%(var)s_cmp)'
            '{%(var)s_cmp=__tmp__; %(var)s=%(rowexpr1)s};'
        ),
        args = args,
    )

def grp_max(maker, args):
    if len(args) != 1:
        raise Exception("'max' function takes 1 argument")
    return maker(
        init = '%(var)s = -10^1000000;',
        update = '__tmp__=%(rowexpr0)s; if(__tmp__>%(var)s){%(var)s=__tmp__};',
        args = args,
    )

def grp_min(maker, args):
    if len(args) != 1:
        raise Exception("'min' function takes 1 argument")
    return maker(
        init = '%(var)s = 10^1000000;',
        update = '__tmp__=%(rowexpr0)s; if(__tmp__<%(var)s){%(var)s=__tmp__};',
        args = args,
    )

def grp_sum(maker, args):
    if len(args) != 1:
        raise Exception("'sum' function takes 1 argument")
    return maker(init='%(var)s = 0;', update='%(var)s += %(rowexpr0)s;', args=args)

def grp_cnt(maker, args):
    if len(args) != 0:
        raise Exception("'cnt' function takes no arguments")
    return maker(init='%(var)s = 0;', update='%(var)s += 1;', args=args)

def grp_avg(maker, args):
    if len(args) != 1:
        raise Exception("'avg' function takes 1 argument")
    return RowExprOp('/', [grp_sum(maker, args), grp_cnt(maker, [])])

FUNC_MAP = {
    'ifmin' : grp_ifmin,
    'ifmax' : grp_ifmax,
    'min' : grp_min,
    'max' : grp_max,
    'sum' : grp_sum,
    'cnt' : grp_cnt,
    'avg' : grp_avg,
}

def parse_grpexpr(grp_ctx, tree, row_ctx, maker):
    if isinstance(tree, _ast.Call) and tree.func.id in FUNC_MAP:
        assert not tree.keywords
        return FUNC_MAP[tree.func.id](
            maker, list(parse_expr(row_ctx, arg) for arg in tree.args)
        )
    else:
        return parse_expr(grp_ctx, tree, partial(parse_grpexpr, maker=maker, row_ctx=row_ctx))

def parse_assign_grpexpr(grp_ctx, tree, row_ctx, maker):
    return parse_assign_expr(grp_ctx, tree, partial(parse_grpexpr, maker=maker, row_ctx=row_ctx))

def find_grp_funcs(ctx):
    func_dict = {}
    for name, val in ctx.itervars():
        for node in val.find(_GrpExprFunc, {}):
            func_dict[node.name] = node
    return func_dict.items()

def awk_grp(data_desc, cond_str, grp_expr_tuples):
    acc_maker = GrpExprFuncMaker('__acc_')
    grp_maker = GrpExprFuncMaker('__grp_')
    cond_ctx = ExprContext(data_desc, substitute_vars=True)
    row_ctx = ExprContext(data_desc)
    acc_ctx = ExprContext(DataDesc([],[]))
    grp_ctx = ExprContext(DataDesc([],[]))
    out_ctx = ExprContext(DataDesc([],[]))
    for grp_type, expr_str in grp_expr_tuples:
        for ast_expr in parse(expr_str).body:
            if grp_type == 'acc':
                expr = parse_assign_grpexpr(acc_ctx, ast_expr, row_ctx, acc_maker)
                cond_ctx.set_var(expr.target, expr.value)
                acc_ctx.set_var(expr.target, expr.value)
                out_ctx.set_var(expr.target, expr.value)
            elif grp_type == 'grp':
                expr = parse_assign_grpexpr(grp_ctx, ast_expr, row_ctx, grp_maker)
                grp_ctx.set_var(expr.target, expr.value)
                out_ctx.set_var(expr.target, expr.value)
            else:
                raise Exception('Unknown grouping type %r' % (grp_type,))

    # parse grouping expr
    cond_namer = Namer('__cond')
    cond_row_namer = Namer('__row_cond')
    cond_exprs = []
    cond_names = []
    cond_row_names = []
    cond_ins_pos = 0
    for node in parse(cond_str).body:
        if isinstance(node, _ast.Assign):
            assert len(node.targets) == 1 and isinstance(node.targets[0], _ast.Name)
            expr = parse_expr(cond_ctx, node.value)
            cond_name = cond_namer.get_name(expr)
            cond_row_name = cond_row_namer.get_name(expr)
            out_ctx.set_var(node.targets[0].id, RowExprVar(out_ctx, cond_name), insert_at=cond_ins_pos)
            cond_ins_pos += 1
        else:
            expr = parse_expr(cond_ctx, node)
            cond_name = cond_namer.get_name(expr)
            cond_row_name = cond_row_namer.get_name(expr)
        cond_exprs.append(expr)
        cond_names.append(cond_name)
        cond_row_names.append(cond_row_name)

    # construct awk script
    print_awk, output_desc = awk_filter_map_from_context(out_ctx)
    assert not print_awk.end

    init_grps = AwkBlock()
    init_accs = AwkBlock()
    calc_row_conds = AwkBlock()
    conds_changed = []
    update_conds = AwkBlock()
    update_grps = AwkBlock()
    update_accs = AwkBlock()
    for cond, name, row_name in zip(cond_exprs, cond_names, cond_row_names):
        calc_row_conds.append(row_name + ' = ' + cond.tostr())
        update_conds.append(name + ' = ' + row_name)
        conds_changed.append(name + '!=' + row_name)
    for name, val in find_grp_funcs(grp_ctx):
        init_grps.append(val.init_str())
        update_grps.append(val.update_str())
    for name, val in find_grp_funcs(acc_ctx):
        init_accs.append(val.init_str())
        update_accs.append(val.update_str())
    conds_changed_str = ' || '.join(conds_changed)

    awk = AwkScript(
        begin = print_awk.begin + init_grps + init_accs,
        end = print_awk.main,
        main = (
            calc_row_conds
            + AwkHeadBlock('if(NR==1)', update_conds)
            + AwkHeadBlock('else', AwkBlock([
                AwkHeadBlock('if(' + conds_changed_str + ')',
                    print_awk.main
                    + update_conds
                    + init_grps
                )])
            )
            + update_grps
            + update_accs
        )
    )

    return awk, output_desc or data_desc

__test__ = dict(
    awk_grp1 = r"""
        >>> from tabkit.header import parse_header
        >>> awk_cmd, output_desc = awk_grp(
        ...     data_desc = parse_header('# d p e s c m'),
        ...     cond_str = 'd;p',
        ...     grp_expr_tuples = [
        ...         ('grp', 'ctr=sum(c)/sum(s); cpm=ctr*avg(m)'),
        ...         ('acc', 'cnt=cnt()'),
        ...         ('grp', 'xctr=avg(c/s)'),
        ...     ],
        ... )
        >>> print awk_cmd.tostr(ident=4, newline='\n')
        BEGIN {
            __grp_5 = 0;
            __grp_4 = 0;
            __grp_1 = 0;
            __grp_0 = 0;
            __grp_3 = 0;
            __grp_2 = 0;
            __acc_0 = 0;
        }
        {
            __row_cond0 = $1;
            __row_cond1 = $2;
            if(NR==1)
            {
                __cond0 = __row_cond0;
                __cond1 = __row_cond1;
            }
            else
            {
                if(__cond0!=__row_cond0 || __cond1!=__row_cond1)
                {
                    ctr = (__grp_0 / __grp_1);
                    print(ctr"\t"(ctr * (__grp_2 / __grp_3))"\t"__acc_0"\t"(__grp_4 / __grp_5));
                    __cond0 = __row_cond0;
                    __cond1 = __row_cond1;
                    __grp_5 = 0;
                    __grp_4 = 0;
                    __grp_1 = 0;
                    __grp_0 = 0;
                    __grp_3 = 0;
                    __grp_2 = 0;
                }
            }
            __grp_5 += 1;
            __grp_4 += ($5 / $4);
            __grp_1 += $4;
            __grp_0 += $5;
            __grp_3 += 1;
            __grp_2 += $6;
            __acc_0 += 1;
        }
        END{
            ctr = (__grp_0 / __grp_1);
            print(ctr"\t"(ctr * (__grp_2 / __grp_3))"\t"__acc_0"\t"(__grp_4 / __grp_5));
        }
        <BLANKLINE>
        """,
    awk_grp2 = r"""
        >>> from tabkit.header import parse_header
        >>> awk_cmd, output_desc = awk_grp(
        ...     data_desc = parse_header('# d p e s c m'),
        ...     cond_str = 'grp=int(_ctr*100)',
        ...     grp_expr_tuples = [
        ...         ('acc', '_ctr=sum(c)/(sum(s)+0.0000001)*100'),
        ...         ('acc', 'm=sprintf("%0.2f",sum(m)/1000000)'),
        ...         ('acc', 'cpm=sprintf("%0.20f",_ctr*m)'),
        ...         ('grp', 'cnt=cnt()'),
        ...     ],
        ... )
        >>> print awk_cmd.tostr(ident=4, newline='\n')
        BEGIN {
            __grp_0 = 0;
            __acc_2 = 0;
            __acc_1 = 0;
            __acc_0 = 0;
        }
        {
            __row_cond0 = int((((__acc_0 / (__acc_1 + 1e-07)) * 100) * 100));
            if(NR==1)
            {
                __cond0 = __row_cond0;
            }
            else
            {
                if(__cond0!=__row_cond0)
                {
                    _ctr = ((__acc_0 / (__acc_1 + 1e-07)) * 100); m = sprintf("%0.2f", (__acc_2 / 1000000));
                    print(__cond0"\t"m"\t"sprintf("%0.20f", (_ctr * m))"\t"__grp_0);
                    __cond0 = __row_cond0;
                    __grp_0 = 0;
                }
            }
            __grp_0 += 1;
            __acc_2 += $6;
            __acc_1 += $4;
            __acc_0 += $5;
        }
        END{
            _ctr = ((__acc_0 / (__acc_1 + 1e-07)) * 100); m = sprintf("%0.2f", (__acc_2 / 1000000));
            print(__cond0"\t"m"\t"sprintf("%0.20f", (_ctr * m))"\t"__grp_0);
        }
        <BLANKLINE>
        """,
)

def _test():
    import doctest
    doctest.testmod()

# if __name__ == "__main__":
#     _test()

### MAIN


import sys
import os
from optparse import OptionParser, Option

pass # from tabkit.header import parse_header, read_fd_header
pass # from tabkit.utils import safe_system, exception_handler, fobj2cmdarg

def main():
    optparser = OptionParser(
        usage = '%prog [options]',
        option_list = [
            Option('--pytrace', dest="pytrace", action="store_true", help="verbose python errors"),
            Option('--print-cmd', dest="print_cmd", action="store_true"),
        ],
    )
    opts, args = optparser.parse_args()

    header = read_fd_header(sys.stdin.fileno())
    data_desc = parse_header(header)
    cmd = 'pv'
    if data_desc.size != None:
        cmd += ' -s %s' % (data_desc.size,)

    os.write(sys.stdout.fileno(), header)
    safe_system(cmd + " " + fobj2cmdarg(sys.stdin))

if __name__ == '__main__':
    exception_handler(main)
