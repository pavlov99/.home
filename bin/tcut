#!/usr/bin/python
# coding: utf-8
# coding: utf-8

### MODULE: common.date

from time import time as get_time
from time import sleep
import time
import datetime

def timedelta( delta ):
    return TimeDelta(
        seconds = delta.seconds,
        days    = delta.days,
    )

class TimeDelta( datetime.timedelta ):
    def total_seconds(self):
        return self.seconds + self.days*24*60*60

def seconds(*args, **nargs):
    d = datetime.timedelta(*args, **nargs)
    return d.seconds + d.days*24*60*60

def date2timestamp(date):
    return time.mktime( date.timetuple() )

def secfmt(seconds):
    int_secs = int(seconds)
    msecs = int((seconds - int_secs)*1000000)
    secs  = int_secs % 60
    mins  = int_secs//60 % 60
    hours = int_secs//60//60 % 24
    days  = int_secs//60//60//24
    if days:
        return "%sd,%02d:%02d:%02d" % (days, hours, mins, secs)
    else:
        return "%02d:%02d:%02d" % (hours, mins, secs)

class StopWatch(object):

    __slots__ = ('value',)

    def __init__(self, value=None):
        self.value = value or get_time()

    def get(self):
        return get_time() - self.value

    def get_and_reset(self):
        new_time = get_time()
        diff = self.value - new_time
        self.value = new_time
        return diff

    def sleep(self, timeout, tick=None):
        """
        Sleep 'timeout' seconds if time elapsed from start is less
        than 'timeout'.

        If 'tick' is specified then sleep will be at most 'tick' seconds
        long, this is to use inside loop when external condition should
        be considered to abort sleeping, return value is boolean telling
        whether we need to sleep more, e.g.

          while not process_killed and stop_watch.sleep(timeout=10, tick=1):
              pass

        """
        diff = self.get()
        if diff < timeout:
            pause = timeout - diff
            if tick != None:
                assert tick > 0
                if tick < pause:
                    pause = tick
            sleep(pause)
            return True
        else:
            return False

class Timer(object):

    def __init__(self, duration):
        self.duration = duration
        self.sw       = StopWatch()

    def time_left(self, max_time=None):
        if max_time == None:
            max_time = self.duration
        time_left = self.duration - self.sw.get()
        if time_left > max_time:
            time_left = max_time
        if time_left <= 0:
            time_left = 0
        return time_left



### MODULE: common.colls

# coding: utf-8

import threading
import weakref
import itertools
import copy
import cPickle
try:
    import hashlib
    md5_func = hashlib.md5
except ImportError:
    import md5
    md5_func = md5.new

class NoEOF(object): pass
def UnpickledStream(fobj, eof=NoEOF):
    while True:
        try:
            item = cPickle.load(fobj)
            if item == eof:
                break
            else:
                yield item
        except EOFError:
            break

def FuncPipe(funcs):
    def pipe(*args, **kwargs):
        for func in funcs:
            args = [func(*args, **kwargs)]
            kwargs = {}
        return args[0]
    return pipe

class CustomObject(object):
    """
    >>> CustomObject(a=1, b=2, c='c').__dict__
    {'a': 1, 'c': 'c', 'b': 2}
    """
    def __init__(self, **nargs):
        for attr, value in nargs.iteritems():
            setattr( self, attr, value )
    def __repr__(self):
        return "%s(**%r)" % (self.__class__.__name__, self.__dict__)

def half_md5(string):
    """
    >>> half_md5('hello world!')
    9787753280732890743L
    """
    return int( md5_func( string ).hexdigest()[16:], 16 )

class PreviewIter:
    """
    >>> pi = PreviewIter([1, 2, 3, 4])
    >>> pi.preview()
    1
    >>> pi.is_empty()
    False
    >>> list(pi)
    [1, 2, 3, 4]
    >>> pi.is_empty()
    True
    """

    def __init__( self, iterable ):
        self.iter = iter( iterable )

    def __iter__(self):
        return self

    def __nonzero__(self):
        return not self.is_empty()

    def next( self ):
        if hasattr( self, '_preview' ):
            item = self._preview
            del self._preview
            return item
        else:
            return self.iter.next()

    def preview( self ):
        if not hasattr( self, '_preview' ):
            self._preview = self.iter.next()
        return self._preview

    def is_empty(self):
        try:
            self.preview()
            return False
        except StopIteration:
            return True

def IterChunkIter( iterable, chunk_size ):
    """
    >>> map(list, IterChunkIter( [1,2,3,4,5,6,7,8], 3 ))
    [[1, 2, 3], [4, 5, 6], [7, 8]]
    """
    iterator = iter( iterable )
    while 1:
        chunk = PreviewIter( itertools.islice( iterator, chunk_size ) )
        if chunk.is_empty():
            break
        yield chunk

def ChunkIter( iterable, chunk_size ):
    """
    >>> list( ChunkIter( [1,2,3,4,5,6,7,8], 3 ) )
    [[1, 2, 3], [4, 5, 6], [7, 8]]
    """
    iterator = iter( iterable )
    while 1:
        chunk = list( itertools.islice( iterator, chunk_size ) )
        if not len( chunk ): break
        yield chunk

class IterWithLength:
    """
    >>> it = (i for i in xrange(5))
    >>> l = IterWithLength(it, 5)
    >>> len(l)
    5
    >>> list(l)
    [0, 1, 2, 3, 4]
    """
    def __init__(self, iterable, len):
        self.iterable = iterable
        self.len = len
        
    def __len__(self,):
        return self.len
    
    def __iter__(self,):
        return iter(self.iterable)

def time_slice(iterable, max_time):
    start = time.time()
    for item in iterable:
        yield item
        if time.time() - start > max_time:
            break

def len_iter(iterable):
    len = 0
    for i in iterable:
        len += 1
    return len

class DefaultDict(dict):
    """
    >>> d = DefaultDict(set)
    >>> d[1].update([1, 2, 3])
    >>> d[1].update([2, 3, 4])
    >>> d
    {1: set([1, 2, 3, 4])}
    >>> d[2]
    set([])
    >>> d
    {1: set([1, 2, 3, 4]), 2: set([])}
    """

    def __init__(self, item_factory, items=None):
        super(DefaultDict, self).__init__(items or [])
        self.item_factory = item_factory

    def __getitem__(self, key):
        try:
            item = super(DefaultDict, self).__getitem__(key)
        except KeyError:
            item = self.item_factory()
            self[key] = item
        return item

class Counter(object):
    """
    >>> counter = Counter()
    >>> int(counter)
    0
    >>> items = counter([1, 2, 3, 4])
    >>> items.__class__.__name__
    'generator'
    >>> list(items)
    [1, 2, 3, 4]
    >>> int(counter)
    4
    """

    def __init__(self):
        self.cnt = 0
    def __int__(self):
        return self.cnt
    def __str__(self):
        return str(self.cnt)
    def get_and_reset(self):
        cnt = self.cnt
        self.cnt = 0
        return cnt
    def __call__(self, items):
        for item in items:
            self.cnt += 1
            yield item

class Singleton(object):
    """ A Pythonic Singleton """
    def __new__(cls, *args, **kwargs):
        if '_inst' not in vars(cls):
            cls._inst = object.__new__(cls, *args, **kwargs)
        return cls._inst

def Memoizer(func, args=[], kwargs={}):
    memo = [None, False]
    lock = threading.Lock()
    def memoized():
        lock.acquire()
        try:
            if not memo[1]:
                memo[0] = func(*args, **kwargs)
                memo[1] = True
        finally:
            lock.release()
        return memo[0]
    return memoized

class RoundRobinArray:
    """
    >>> rra = RoundRobinArray(3)
    >>> rra.put(1)
    >>> list(rra)
    [1]
    >>> rra.put(2)
    >>> rra.put(3)
    >>> list(rra)
    [1, 2, 3]
    >>> rra.put(4)
    >>> list(rra)
    [2, 3, 4]
    """

    def __init__( self, size ):
        self.array   = []
        self.end_ptr = 0
        self.size    = size

    def put( self, item ):
        if len( self.array ) < self.size:
            self.array.append( item )
        else:
            self.array[ self.end_ptr ] = item
        self.end_ptr = ( self.end_ptr + 1 ) % self.size

    def __iter__(self):
        return iter( self.array[self.end_ptr:] + self.array[:self.end_ptr] )

def progress_bar(iterator, precision=None, total=None, period=1):
    """ Shortcut for using ProgressBar """
    return ProgressBar(precision, period)(iterator, total)

import sys, time
pass # from common.date import secfmt

def ProgressBar(precision=None, period=1):
    """ Useful to monitor long running console processes """

    precision = precision
    period = period
    gettime = time.time

    def ProgressBarInstance(iterable, total=None):
        speed = 0
        cnt = 0
        started = gettime()
        last_report_time = 0
        last_report_cnt = 0
        if total == None and hasattr(iterable, '__len__'):
            total = len(iterable)

        def time_to_log():
            if precision == None:
                if gettime() - last_report_time >= period:
                    return True
            else:
                if cnt % precision == 0:
                    return True
            return False

        for item in iterable:
            yield item
            cnt += 1

            if time_to_log():
                curtime = gettime()
                speed = (cnt + 1) / (curtime - started)
                mspeed = (cnt - last_report_cnt) / (curtime - last_report_time)
                msg = "%8d (%0.3f ips, %0.3f current)" % (cnt, speed, mspeed)
                if total and speed:
                    msg += " %0.1f%% (left %s of %s)" % (
                        100 * float(cnt + 1) / total,
                        secfmt((total - cnt) / speed),
                        secfmt(total / speed),
                    )
                print >> sys.stderr, "progress_bar:", msg
                last_report_time = curtime
                last_report_cnt = cnt

        speed = (cnt + 1) / (gettime() - started)
        print >> sys.stderr, "progress_bar: summary %d in %s at %0.3f ips" % (
            cnt + 1,
            secfmt(gettime() - started),
            speed,
        )

    return ProgressBarInstance

class SummableSet(set):
    """
    >>> SummableSet([1, 2, 3]) + SummableSet([2, 3, 4])
    SummableSet([1, 2, 3, 4])
    >>> s = SummableSet([1, 2, 3])
    >>> s += SummableSet([2, 3, 4])
    >>> s
    SummableSet([1, 2, 3, 4])
    """

    def __iadd__(self, set2):
        self.update(set2)
        return self

    def __add__(self, set2):
        return self.union(iter(set2))

    __radd__ = __add__

def accum(pairs):
    return Accum.from_pairs(pairs)

class Accum(dict):
    """
    >>> Accum({1:2, 2:3})
    {1: 2, 2: 3}

    >>> a = Accum([1, 2, 1, 2, 3])
    >>> a
    {1: 2, 2: 2, 3: 1}
    >>> a.accum({3:9, 2:18})
    >>> a.accum([3, 2, 2])
    >>> a
    {1: 2, 2: 22, 3: 11}
    >>> a.total()
    35
    >>> for key, cnt, rel in a.iternormal():
    ...     print "%s %3s %0.3f" % (key, cnt, rel)
    1   2 0.057
    2  22 0.629
    3  11 0.314

    >>> a = Accum.from_pairs((
    ...     ('a', Accum([1,1,2,2])     ),
    ...     ('s', SummableSet([1,2,3]) ),
    ...     ('a', Accum([1,2,2,3])     ),
    ...     ('s', SummableSet([2,3,4]) ),
    ... ))
    >>> a
    {'a': {1: 3, 2: 4, 3: 1}, 's': SummableSet([1, 2, 3, 4])}
    """

    def __init__(self, values=None):
        super(Accum, self).__init__()
        if values:
            self.accum(values)

    @classmethod
    def from_pairs(cls, values):
        self = cls()
        for value, cnt in values:
            self.add(value, cnt)
        return self

    def add(self, value, cnt=1):
        try:
            self[value] += cnt
        except KeyError:
            self[value] = cnt

    def accum(self, values):
        self += values

    def accum_pairs(self, values):
        for value, cnt in values:
            self.add(value, cnt)

    def __iadd__(self, values):
        if isinstance(values, dict):
            for value, cnt in values.iteritems():
                try:
                    self[value] += cnt
                except KeyError:
                    self[value] = cnt
        else:
            for val in values:
                self[val] = self.get(val, 0) + 1
        return self

    def __add__(self, stat2):
        stat = Accum()
        if self != 0:
            stat += self
        if stat2 != 0:
            stat += stat2
        return stat
    __radd__ = __add__

    def total(self):
        return sum(self.itervalues())

    def iternormal(self):
        total = 0.0
        if self:
            total = float(sum(self.itervalues()))
        for key, val in self.iteritems():
            yield key, val, val/total

class StatObject(CustomObject):
    """
    >>> s1 = StatObject(a=1,b=2)
    >>> s2 = StatObject(a=3,b=4,c=5,d=Accum([1,1,2,2,3,3,3]))
    >>> s3 = s1+s2

    >>> s3.__dict__
    {'a': 4, 'c': 5, 'b': 6, 'd': {1: 2, 2: 2, 3: 3}}

    >>> s1.__dict__
    {'a': 1, 'b': 2}

    >>> s2.__dict__
    {'a': 3, 'c': 5, 'b': 4, 'd': {1: 2, 2: 2, 3: 3}}

    >>> s2 += s3
    >>> s2.__dict__
    {'a': 7, 'c': 10, 'b': 10, 'd': {1: 4, 2: 4, 3: 6}}
    """

    def __add__(self, stat2):
        stat = StatObject()
        if self != 0:
            stat += self
        if stat2 != 0:
            stat += stat2
        return stat
    __radd__ = __add__

    def __iadd__(self, data):
        if not isinstance(data, StatObject):
            class_name = self.__class__.__name__
            raise TypeError(
                'can only add %s (not "%s") to %s' % (
                    class_name,
                    data.__class__.__name__,
                    class_name,
                )
            )
        for key, val in data.__dict__.iteritems():
            try:
                curr_val = getattr(self, key)
            except AttributeError:
                setattr(self, key, copy.deepcopy(val))
            else:
                setattr(self, key, curr_val + val)
        return self

class _Sentinel(object):
    pass

def layer_merge_ordered(iterables, null=None, order_func=None):
    """
    Склеивает отсортированные последовательности.
    Сохраняя информацию о том, какой элемент из какой последовательности
    пришел.

    >>> list(layer_merge_ordered((
    ...     [1, 3, 5],
    ...     [2, 4, 6],
    ...     [3, 4, 5],
    ... ), null=0))
    [[1, 0, 0], [0, 2, 0], [3, 0, 3], [0, 4, 4], [5, 0, 5], [0, 6, 0]]
    """

    sentinel = _Sentinel()
    def next_if_any(iterator):
        try:
            return iterator.next()
        except StopIteration:
            return sentinel

    order_func = order_func or ( lambda item: item )
    iterators  = [ iter(iterable) for iterable in iterables ]
    items      = [ next_if_any(iterator) for iterator in iterators ]
    while sum( int(item != sentinel) for item in items ):
        min_order = min(order_func(item) for item in items if item != sentinel)
        layer     = []
        for i, item in enumerate( items ):
            if item != sentinel and order_func(item) == min_order:
                layer.append( item )
                items[i] = next_if_any( iterators[i] )
            else:
                layer.append( null )
        yield layer

def merge_ordered(iterables, key=None):
    """
    Склеивает отсортированные последовательности.

    >>> list(merge_ordered((
    ...     [1, 3],
    ...     [2, 4, 6],
    ...     [3, 4, 5, 6, 7],
    ... )))
    [1, 2, 3, 3, 4, 4, 5, 6, 6, 7]
    """

    key = key or ( lambda item: item )
    iters = []
    for iterable in iterables:
        iterator = PreviewIter(iterable)
        try:
            iterator.preview()
            iters.append(iterator)
        except StopIteration:
            pass
    iters.sort(key=lambda iterator: key(iterator.preview()))
    iters_len = len(iters)

    while iters_len > 1:
        yield iters[0].next()
        try:
            new_item = iters[0].preview()
            new_item_key = key(new_item)
            if new_item_key > key(iters[1].preview()):
                free_iterator = iters.pop(0)
                for iter_num, iterator in enumerate(iters):
                    if new_item_key < key(iterator.preview()):
                        iters.insert(iter_num, free_iterator)
                        free_iterator = None
                        break
                if free_iterator:
                    iters.append(free_iterator)
        except StopIteration:
            del iters[0]
            iters_len -= 1

    if iters_len:
        for item in iters[0]:
            yield item

class List(object):

    @classmethod
    def merge_ordered(cls, lists, null=None, order_func=None):
        return layer_merge_ordered(lists, null, order_func)

    @classmethod
    def diff_ordered( cls, list1, list2, attr=None ):
        added   = []
        removed = []

        idx1 = 0;
        idx2 = 0;

        while idx1 < len( list1 ) or idx2 < len( list2 ):
            if  idx2 >= len( list2 ):
                removed.append( list1[idx1] )
                idx1 += 1
            elif idx1 >= len( list1 ):
                added.append( list2[idx2] )
                idx2 += 1
            elif cls._cmp( list1[idx1], list2[idx2], attr ) == -1:
                removed.append( list1[idx1] )
                idx1 += 1
            elif cls._cmp( list1[idx1], list2[idx2], attr ) == 1:
                added.append( list2[idx2] )
                idx2 += 1
            else:
                idx1 += 1
                idx2 += 1

        return added, removed

    @staticmethod
    def _cmp( obj1, obj2, attr ):
        if attr:
            return cmp( getattr( obj1, attr ), getattr( obj2, attr ) )
        else:
            return cmp( obj1, obj2 )

class ObjCache:

    def __init__( self, size, keys ):
        self.lock  = threading.Lock()
        self.items = RoundRobinArray( size )
        self.dicts = {}
        for key in keys:
            self.dicts[ key ] = weakref.WeakValueDictionary()

    def get( self, key, value ):
        item = None
        self.lock.acquire()
        try:
            item = self.dicts[ key ][ value ]
        except KeyError:
            pass
        self.lock.release()
        return item

    def add( self, item ):
        self.lock.acquire()
        try:
            self.items.put( item )
            for key, dct in self.dicts.iteritems():
                dct[ getattr( item, key ) ] = item
        finally:
            self.lock.release()

    def has_key( self, key ):
        self.lock.acquire()
        res = self.dicts.has_key( key )
        self.lock.release()
        return res

    def cached_objects( self, key=None ):
        self.lock.acquire()
        if not key:
            key = self.dicts.iterkeys().next()
        objects = self.dicts[ key ].values()
        self.lock.release()
        return objects

    def keys( self ):
        self.lock.acquire()
        keys = self.dicts.keys()
        self.lock.release()
        return keys

    def reset( self ):
        self.lock.acquire()
        self.items = RoundRobinArray( self.items.size )
        for key in self.dicts.keys():
            self.dicts[ key ] = weakref.WeakValueDictionary()
        self.lock.release()

### MODULE: common.namedtuple

try:
    from collections import namedtuple
except ImportError:
    from operator import itemgetter as _itemgetter
    from keyword import iskeyword as _iskeyword
    import sys as _sys

    def namedtuple(typename, field_names, verbose=False, rename=False):
        """Returns a new subclass of tuple with named fields.

        >>> Point = namedtuple('Point', 'x y')
        >>> Point.__doc__                   # docstring for the new class
        'Point(x, y)'
        >>> p = Point(11, y=22)             # instantiate with positional args or keywords
        >>> p[0] + p[1]                     # indexable like a plain tuple
        33
        >>> x, y = p                        # unpack like a regular tuple
        >>> x, y
        (11, 22)
        >>> p.x + p.y                       # fields also accessable by name
        33
        >>> d = p._asdict()                 # convert to a dictionary
        >>> d['x']
        11
        >>> Point(**d)                      # convert from a dictionary
        Point(x=11, y=22)
        >>> p._replace(x=100)               # _replace() is like str.replace() but targets named fields
        Point(x=100, y=22)

        """

        # Parse and validate the field names.  Validation serves two purposes,
        # generating informative error messages and preventing template injection attacks.
        if isinstance(field_names, basestring):
            field_names = field_names.replace(',', ' ').split() # names separated by whitespace and/or commas
        field_names = tuple(map(str, field_names))
        if rename:
            names = list(field_names)
            seen = set()
            for i, name in enumerate(names):
                if (not min(c.isalnum() or c=='_' for c in name) or _iskeyword(name)
                    or not name or name[0].isdigit() or name.startswith('_')
                    or name in seen):
                        names[i] = '_%d' % i
                seen.add(name)
            field_names = tuple(names)
        for name in (typename,) + field_names:
            if not min(c.isalnum() or c=='_' for c in name):
                raise ValueError('Type names and field names can only contain alphanumeric characters and underscores: %r' % name)
            if _iskeyword(name):
                raise ValueError('Type names and field names cannot be a keyword: %r' % name)
            if name[0].isdigit():
                raise ValueError('Type names and field names cannot start with a number: %r' % name)
        seen_names = set()
        for name in field_names:
            if name.startswith('_') and not rename:
                raise ValueError('Field names cannot start with an underscore: %r' % name)
            if name in seen_names:
                raise ValueError('Encountered duplicate field name: %r' % name)
            seen_names.add(name)

        # Create and fill-in the class template
        numfields = len(field_names)
        argtxt = repr(field_names).replace("'", "")[1:-1]   # tuple repr without parens or quotes
        reprtxt = ', '.join('%s=%%r' % name for name in field_names)
        template = '''class %(typename)s(tuple):
        '%(typename)s(%(argtxt)s)' \n
        __slots__ = () \n
        _fields = %(field_names)r \n
        def __new__(_cls, %(argtxt)s):
            return _tuple.__new__(_cls, (%(argtxt)s)) \n
        @classmethod
        def _make(cls, iterable, new=tuple.__new__, len=len):
            'Make a new %(typename)s object from a sequence or iterable'
            result = new(cls, iterable)
            if len(result) != %(numfields)d:
                raise TypeError('Expected %(numfields)d arguments, got %%d' %% len(result))
            return result \n
        def __repr__(self):
            return '%(typename)s(%(reprtxt)s)' %% self \n
        def _asdict(self):
            'Return a new dict which maps field names to their values'
            return dict(zip(self._fields, self)) \n
        def _replace(_self, **kwds):
            'Return a new %(typename)s object replacing specified fields with new values'
            result = _self._make(map(kwds.pop, %(field_names)r, _self))
            if kwds:
                raise ValueError('Got unexpected field names: %%r' %% kwds.keys())
            return result \n
        def __getnewargs__(self):
            return tuple(self) \n\n''' % locals()
        for i, name in enumerate(field_names):
            template += '        %s = _property(_itemgetter(%d))\n' % (name, i)
        if verbose:
            print template

        # Execute the template string in a temporary namespace
        namespace = dict(_itemgetter=_itemgetter, __name__='namedtuple_%s' % typename,
                         _property=property, _tuple=tuple)
        try:
            exec template in namespace
        except SyntaxError, e:
            raise SyntaxError(e.message + ':\n' + template)
        result = namespace[typename]

        # For pickling to work, the __module__ variable needs to be set to the frame
        # where the named tuple is created.  Bypass this step in enviroments where
        # sys._getframe is not defined (Jython for example) or sys._getframe is not
        # defined for arguments greater than 0 (IronPython).
        try:
            result.__module__ = _sys._getframe(1).f_globals.get('__name__', '__main__')
        except (AttributeError, ValueError):
            pass

        return result

_TestPoint = namedtuple('_TestPoint', 'x, y')
def test_pickle():
    """
    Verify that instances can be pickled

    >>> from cPickle import loads, dumps
    >>> p = _TestPoint(x=10, y=20)
    >>> p == loads(dumps(p, -1))
    True
    """
    pass

def test_override():
    """
    Test and demonstrate ability to override methods

    >>> class Point(namedtuple('Point', 'x y')):
    ...     @property
    ...     def hypot(self):
    ...         return (self.x ** 2 + self.y ** 2) ** 0.5
    ...     def __str__(self):
    ...         return 'Point: x=%6.3f y=%6.3f hypot=%6.3f' % (self.x, self.y, self.hypot)
    ...
    >>> for p in Point(3,4), Point(14,5), Point(9./7,6):
    ...     print p
    Point: x= 3.000 y= 4.000 hypot= 5.000
    Point: x=14.000 y= 5.000 hypot=14.866
    Point: x= 1.286 y= 6.000 hypot= 6.136
    """
    pass

def test_replace():
    """
    >>> class Point(namedtuple('Point', 'x y')):
    ...     'Point class with optimized _make() and _replace() without error-checking'
    ...     _make = classmethod(tuple.__new__)
    ...     def _replace(self, _map=map, **kwds):
    ...         return self._make(_map(kwds.get, ('x', 'y'), self))
    >>> print Point(11, 22)._replace(x=100)
    Point(x=100, y=22)
    """
    pass

def _test():
    import doctest
    doctest.testmod()

# if __name__ == "__main__":
#     _test()
# 

### MODULE: common.safe_popen

from subprocess import Popen, PIPE

def safe_popen_args(command):
    return ['/bin/bash', '-o', 'pipefail', '-o', 'errexit', '-c', command]

class SafePopenError(Exception):
    pass

class SafePopen(Popen):
    def __init__(self, cmdline, bufsize=None):
        popen_args = dict(
            args = safe_popen_args(cmdline),
            shell = False,
            stdout = PIPE,
        )
        if bufsize != None:
            popen_args['bufsize'] = bufsize
        super(SafePopen, self).__init__(**popen_args)
        self.__cmdline = cmdline

    def close(self):
        self.stdout.close()
        status = self.wait()
        if status != 0:
            raise SafePopenError("safe_popen failed on %r, status = %r" % (
                self.__cmdline, status
            ))

def safe_popen(command, bufsize=None):
    """
    >>> list(safe_popen('echo ok'))
    ['ok\\n']

    >>> list(safe_popen('false; echo ok'))
    Traceback (most recent call last):
        ...
    Exception: safe_popen failed on 'false; echo ok', status = 1

    >>> list(safe_popen('false|true; echo ok'))
    Traceback (most recent call last):
        ...
    Exception: safe_popen failed on 'false|true; echo ok', status = 1
    """
    popen = SafePopen(command, bufsize)
    try:
        for line in popen.stdout:
            yield line
    except:
        popen.close()
        raise
    else:
        popen.close()

def safe_system(command):
    popen = Popen(args=safe_popen_args(command), shell=False)
    status = popen.wait()
    if status != 0:
        raise SafePopenError("safe_system failed on %r, status = %r" % (command, status))


### MODULE: tabkit.datasrc

import os
from itertools import izip
pass # from common.namedtuple import namedtuple

TYPES = set([
    'float',
    'int',
    'str',
    'bool',
    'any',
])

class DataField(namedtuple('DataField', 'name type')):
    def __repr__(self):
        return "%s(%r, %r)" % (
            self.__class__.__name__, self.name, self.type
        )

class SortType(object):
    STRING = 'str'
    NUMERIC = 'num'
    GENERAL_NUMERIC = 'general'
    HUMAN_NUMERIC = 'human'
    MONTH = 'month'

    @classmethod
    def is_valid(self, sort_type):
        return sort_type in (
            self.STRING,
            self.NUMERIC,
            self.GENERAL_NUMERIC,
            self.HUMAN_NUMERIC,
            self.MONTH,
        )

class DataFieldOrder(object):

    def __init__(self, name, sort_type=None, desc=None):
        self.name = name
        self.desc = desc
        self.sort_type = sort_type
        if self.desc == None:
            self.desc = False
        if self.sort_type == None:
            self.sort_type = SortType.STRING
        if not SortType.is_valid(self.sort_type):
            raise Exception('Unknown sort type %r' % (self.sort_type,))

    def __eq__(self, other):
        return (
            self.name == other.name
            and self.sort_type == other.sort_type
            and self.desc == other.desc
        )

    def __ne__(self, other):
        return not (self == other)

    def __repr__(self):
        return "%s(%r, sort_type=%r, desc=%r)" % (
            self.__class__.__name__, self.name, self.sort_type, self.desc
        )

def copy_field_order(field_order, name=None):
    return DataFieldOrder(
        name = name or field_order.name,
        sort_type = field_order.sort_type,
        desc = field_order.desc,
    )

def unix_sort_flags(field_order):
    sort_flags = ''

    if field_order.sort_type == SortType.NUMERIC:
        sort_flags += 'n'
    elif field_order.sort_type == SortType.GENERAL_NUMERIC:
        sort_flags += 'g'
    elif field_order.sort_type == SortType.HUMAN_NUMERIC:
        sort_flags += 'h'
    elif field_order.sort_type == SortType.MONTH:
        sort_flags += 'M'

    if field_order.desc:
        sort_flags += 'r'

    return sort_flags

def header_order_field(order_field):
    order_str = order_field.name
    if order_field.desc:
        order_str += ':desc'
    if order_field.sort_type == SortType.NUMERIC:
        order_str += ':num'
    if order_field.sort_type == SortType.GENERAL_NUMERIC:
        order_str += ':general'
    if order_field.sort_type == SortType.HUMAN_NUMERIC:
        order_str += ':human'
    if order_field.sort_type == SortType.MONTH:
        order_str += ':month'
    return order_str

def field_order_from_header(name, modifiers):
    desc = None
    sort_type = None
    for modifier in modifiers:
        if SortType.is_valid(modifier):
            if sort_type != None:
                raise Exception('Conflicting sort types %r and %r' % (sort_type, modifier))
            sort_type = modifier
        elif modifier in ('asc', 'desc'):
            if desc != None:
                raise Exception('Ambiguous order direction in %r' % (order_field_str,))
            if modifier == 'asc':
                desc = False
            else:
                desc = True
        else:
            raise Exception('Unknown order modifier %r' % (modifier,))
    return DataFieldOrder(name, sort_type, desc)

class DataOrder(object):
    def __init__(self, data_order):
        self.data_order = data_order

    def fields_are_ordered(self, field_names):
        for field_name, order in zip(field_names, self.data_order):
            if field_name != order.name:
                return False
        return True

    def is_ordered_by(self, data_order):
        if isinstance(data_order, DataOrder):
            data_order = data_order.data_order
        data_order = list(data_order)
        if len(self.data_order) < len(data_order):
            return False
        for required, actual in zip(data_order, self.data_order):
            if required != actual:
                return False
        return True

    def __nonzero__(self):
        return bool(self.data_order)

    def __iter__(self):
        return iter(self.data_order)

    def __repr__(self):
        return "%s(%r)" % (
            self.__class__.__name__, self.data_order
        )

class DataDesc(object):
    def __init__(self, fields, order=None, size=None):
        self.size = size
        self.fields = fields
        if isinstance(order, DataOrder):
            self.order = order
        else:
            self.order = DataOrder(order or [])
        self.field_names = dict((field.name, idx) for idx, field in enumerate(self.fields))
        if len(self.fields) != len(self.field_names):
            raise Exception("Conflicting field names in %r" % (self.fields,))
        for order_field in self.order:
            if order_field.name not in self.field_names:
                raise Exception('Unknown ordering field name %r' % (order_field.name,))

    def __getitem__(self, idx):
        assert isinstance(idx, slice)
        fields = self.fields[idx]
        field_names = list(field.name for field in fields)
        order = list(self.order)
        for order_field in order:
            if order_field.name not in field_names:
                break
            order.append(order_field)
        return DataDesc(fields, order, self.size)

    def __add__(self, desc):
        return paste_data_desc(self, desc)

    def __radd__(self, desc):
        return paste_data_desc(desc, self)

    def field_index(self, name):
        return self.field_names[name]

    def get_field(self, name):
        return self.fields[self.field_names[name]]

    def has_field(self, name):
        return name in self.field_names

    def __repr__(self):
        if self.order and self.size == None:
            return "%s(%r, %r)" % (
                self.__class__.__name__, self.fields, self.order
            )
        if not self.order and self.size == None:
            return "%s(%r)" % (
                self.__class__.__name__, self.fields
            )
        return "%s(%r, %r, %r)" % (
            self.__class__.__name__, self.fields, self.order, self.size
        )

def make_data_field(field):
    if isinstance(field, DataField):
        return field
    if isinstance(field, tuple) and isinstance(field[0], str) and field[1] in TYPES:
        return DataField(*field)
    raise Exception("Can't convert %r to DataField" % (field,))

def make_data_desc(desc):
    if isinstance(desc, tuple) and isinstance(desc[0], str) and desc[1] in TYPES:
        return DataDesc([DataField(*desc)])
    if isinstance(desc, list):
        return DataDesc(list(make_data_field(item) for item in desc))
    if isinstance(desc, DataDesc):
        return desc
    try:
        return DataDesc([make_data_field(desc)])
    except Exception, err:
        pass
    raise Exception("Can't convert %r to DataDesc" % (desc,))

def paste_data_desc(d1, d2):
    """
    >>> from tabkit.header import parse_header

    >>> parse_header('# a c:int')[:1] + ('b', 'str')
    DataDesc([DataField('a', 'any'), DataField('b', 'str')])

    >>> parse_header('# a b:int') + ('b', 'str')
    Traceback (most recent call last):
        ...
    Exception: Conflicting fields ['b'] in DataDesc([DataField('a', 'any'), DataField('b', 'int')]) and DataDesc([DataField('b', 'str')])

    >>> parse_header('# a #ORDER: a #SIZE: 2') + [('b', 'str')]
    DataDesc([DataField('a', 'any'), DataField('b', 'str')], DataOrder([DataFieldOrder('a', sort_type='str', desc=False)]))

    >>> ('b','str') + parse_header('# a #ORDER: a #SIZE: 2')
    DataDesc([DataField('b', 'str'), DataField('a', 'any')], DataOrder([DataFieldOrder('a', sort_type='str', desc=False)]))
    """
    d1 = make_data_desc(d1)
    d2 = make_data_desc(d2)
    isect = list(set(field.name for field in d1.fields).intersection(
        field.name for field in d2.fields
    ))
    if isect:
        raise Exception("Conflicting fields %r in %r and %r" % (isect, d1, d2))
    if d1.size == None or d2.size == None:
        size = None
    else:
        size = d1.size + d2.size
    return DataDesc(
        fields = d1.fields + d2.fields,
        order = d1.order or d2.order,
        size = size,
    )

def merge_data_desc(desc1, desc2):
    return DataDesc(
        fields = merge_data_fields(desc1.fields, desc2.fields),
        order = merge_data_order(desc1.order, desc2.order),
    )

def merge_data_fields(fields1, fields2):
    if len(fields1) != len(fields2):
        raise Exception('Incompatible data fields: %r and %r' % (
            fields1, fields2,
        ))

    fields = []
    for field1, field2 in zip(fields1, fields2):
        if field1.name != field2.name:
            raise Exception('Incompatible data fields: %r and %r' % (
                fields1, fields2,
            ))
        if field1.type == field2.type:
            fields.append(DataField(field1.name, field1.type))
        elif field1.type == 'any':
            fields.append(DataField(field1.name, field2.type))
        elif field2.type == 'any':
            fields.append(DataField(field1.name, field1.type))
        else:
            raise Exception('Incompatible data fields: %r and %r' % (
                fields1, fields2,
            ))

    return fields

def merge_data_order(order1, order2):
    order = []
    for ord1, ord2 in zip(order1, order2):
        if ord1 == ord2:
            order.append(copy_field_order(ord1))
        else:
            break
    return DataOrder(order)

def rename_fields(desc, renamings):
    new_fields = []
    for field in desc.fields:
        if field.name in renamings:
            new_fields.append(DataField(renamings[field.name], field.type))
        else:
            new_fields.append(DataField(field.name, field.type))

    new_order = []
    for field in desc.order:
        if field.name in renamings:
            new_order.append(copy_field_order(
                field, name=renamings[field.name],
            ))
        else:
            new_order.append(copy_field_order(
                field, name=field.name,
            ))

    return DataDesc(new_fields, new_order)

def convertible(from_type, to_type):
    if from_type == 'any' or to_type == 'any':
        return True
    if to_type == from_type:
        return True
    if to_type == 'str':
        return True
    if to_type == 'float' and from_type in ('int', 'bool'):
        return True
    if to_type == 'int' and from_type == 'bool':
        return True
    return False

def _test():
    import doctest
    doctest.testmod()

# if __name__ == "__main__":
#     _test()

### MODULE: tabkit.header

import os
import re

pass # from tabkit.datasrc import DataField, DataFieldOrder, DataDesc, SortType
pass # from tabkit.datasrc import header_order_field, field_order_from_header

def field_split(fields_str):
    return re.split(r'[;,\s]+', fields_str.strip())

def parse_header_order(zone_fields):
    for order_field_str in zone_fields:
        order_field = order_field_str.strip().split(':')
        yield field_order_from_header(order_field[0], order_field[1:])

def parse_header(header):
    """
    >>> header = "# shows:int clicks:int ctr:float rel url #ORDER: url:asc, ctr:desc:num"
    >>> parse_header(header) #doctest: +NORMALIZE_WHITESPACE
    DataDesc([DataField('shows', 'int'),
            DataField('clicks', 'int'),
            DataField('ctr', 'float'),
            DataField('rel', 'any'),
            DataField('url', 'any')],
        DataOrder([DataFieldOrder('url', sort_type='str', desc=False),
            DataFieldOrder('ctr', sort_type='num', desc=True)]))
    """
    if not header.startswith('#'):
        raise Exception('Bad header')
    else:
        data_fields = []
        data_order = []
        data_size = None

        zones = header[1:].split('#')
        fields_str = zones[0]
        fields = field_split(fields_str)
        for field in fields:
            field_parts = field.split(':')
            field_name = field_parts[0]
            field_type = 'any'
            if len(field_parts) == 2:
                field_type = field_parts[1]
            elif len(field_parts) > 2:
                raise Exception('Invalid field %r' % (field,))
            data_fields.append(DataField(field_name, field_type))

        for zone in zones[1:]:
            zone_name, zone_data = zone.strip().split(None, 1)
            if zone_name == 'ORDER:':
                data_order = list(parse_header_order(field_split(zone_data)))
            elif zone_name == 'SIZE:':
                data_size = int(zone_data)
            else:
                raise Exception('Bad header, invalid zone %r' % (zone_name,))

        return DataDesc(data_fields, data_order, data_size)

def make_header(data_desc):
    r"""
    >>> desc = DataDesc(
    ...     [DataField('shows', 'int'), DataField('url', 'any')],
    ...     [
    ...         DataFieldOrder('url', desc=True),
    ...         DataFieldOrder('shows', sort_type=SortType.NUMERIC),
    ...     ],
    ... )
    >>> make_header(desc)
    '# shows:int\turl #ORDER: url:desc\tshows:num\n'
    """
    fields = []
    for field in data_desc.fields:
        if field.type == 'any':
            fields.append(field.name)
        else:
            fields.append(field.name + ':' + field.type)
    order = []
    for order_field in data_desc.order:
        order.append(header_order_field(order_field))
    header = '# ' + '\t'.join(fields)
    if order:
        header += ' #ORDER: ' + '\t'.join(order)
    if data_desc.size != None:
        header += ' #SIZE: ' + str(data_desc.size)
    header += "\n"
    return header

def read_fd_header(fd):
    header = ''
    while header[-1:] != '\n':
        ch = os.read(fd, 1)
        if ch == '':
            break
        header += ch
    return header

def read_file_header(fname):
    fobj = open(fname)
    try:
        header = fobj.readline()
    finally:
        fobj.close()
    return header

def _test():
    import doctest
    doctest.testmod()

# if __name__ == "__main__":
#     _test()

### MODULE: tabkit.utils

import sys
import os
import gzip
from subprocess import Popen, PIPE
from collections import defaultdict
from pipes import quote
from itertools import islice

pass # from tabkit.header import parse_header, read_fd_header, read_file_header
pass # from tabkit.datasrc import DataDesc, merge_data_fields
pass # from common.safe_popen import safe_popen, safe_system

try:
    from functools import partial
except ImportError:
    def partial(func, *args, **keywords):
        def newfunc(*fargs, **fkeywords):
            newkeywords = keywords.copy()
            newkeywords.update(fkeywords)
            return func(*(args + fargs), **newkeywords)
        newfunc.func = func
        newfunc.args = args
        newfunc.keywords = keywords
        return newfunc

def exception_handler(func):
    if '--pytrace' in sys.argv:
        func()
    else:
        try:
            func()
        except SyntaxError, err:
            msg, (fname, line, col, code) = err.args
            err = "%s in %r (line=%s, column=%s)" % (msg, code, line, col)
            print >> sys.stderr, sys.argv[0] + ":", type(err).__name__ + ":", err
        except Exception, err:
            print >> sys.stderr, sys.argv[0] + ":", type(err).__name__ + ":", err
            sys.exit(1)

class InputFile(object):
    def __init__(self, header):
        self.header = header
    def desc(self):
        return parse_header(self.header)
    def cmd_arg(self):
        raise Exception('Redefine me')
    def is_stdin(self):
        raise Exception('Redefine me')

class PlainFile(InputFile):
    def __init__(self, fname, header=None):
        self.fname = fname
        self.header = header or read_file_header(fname)
        self.has_header = not header
    def cmd_arg(self):
        if self.has_header:
            return "<(tail -qn +2 %s || kill $$)" % (quote(self.fname),)
        else:
            return quote(self.fname)
    def is_stdin(self):
        return False

class GzipFile(InputFile):
    def __init__(self, fname, header=None):
        self.fname = fname
        self.has_header = not header
        if not self.has_header:
            self.header = header
        else:
            fobj = gzip.open(fname)
            try:
                self.header = fobj.readline()
            finally:
                fobj.close()
    def cmd_arg(self):
        if self.has_header:
            return "<(set -o pipefail; gzip -cd %s|tail -qn +2 || kill $$)" % (quote(self.fname),)
        else:
            return "<(gzip -cd %s || kill $$)" % (quote(self.fname),)
    def is_stdin(self):
        return False

class FdFile(InputFile):
    def __init__(self, fname, fd, header=None):
        self.fname = '/dev/fd/%d' % (fd,)
        self.fd = fd
        self.header = header or read_fd_header(fd)
    def cmd_arg(self):
        return '/dev/fd/%d' % (self.fd,)
    def is_stdin(self):
        return self.fd == 0

class StreamFile(FdFile):
    def __init__(self, fname, fobj, header=None):
        super(StreamFile, self).__init__(fname, fobj.fileno(), header)
        self.fobj = fobj

class StdinFile(StreamFile):
    def __init__(self, header=None):
        super(StdinFile, self).__init__('-', sys.stdin, header)

def input_file_from_cmdline_arg(fname, header=None, gzip=False):
    if fname == '-':
        return StdinFile(header)
    elif os.path.isfile(fname):
        if gzip:
            return GzipFile(fname, header)
        else:
            return PlainFile(fname, header)
    elif os.path.exists(fname):
        if fname.startswith('/dev/fd/'):
            return FdFile(fname, int(fname.split('/', 3)[3]), header)
        else:
            return StreamFile(fname, open(fname), header)
    else:
        raise Exception('File does not exist: %r' % (fname,))

class FilesList(object):
    def __init__(self, fnames, stdin_fallback=True, header=None, gzip=False):
        self.header = header
        self.input_files = []

        if stdin_fallback and not fnames:
            fnames = ['-']

        got_stdin = False
        for fname in fnames:
            input_file = input_file_from_cmdline_arg(fname, header, gzip=gzip)
            if input_file.is_stdin():
                if got_stdin:
                    raise Exception('"-" specified as input file more than once')
                got_stdin = True
            self.input_files.append(input_file)

    def __len__(self):
        return len(self.input_files)

    def __iter__(self):
        return iter(self.input_files)

    def get_size(self):
        size = 0
        for fname, desc in self.names_descs():
            if os.path.isfile(fname):
                size += os.stat(fname).st_size
            elif desc.size != None:
                size += desc.size
            else:
                return None
        return size

    def names_descs(self):
        for ifile in self.input_files:
            yield ifile.fname, ifile.desc()

    def concat_desc(self):
        fields = None
        order = []
        for fname, desc in self.names_descs():
            order = desc.order
            if fields:
                fields = merge_data_fields(fields, desc.fields)
            else:
                fields = desc.fields
        if len(self) != 1:
            order = []
        return DataDesc(fields, order)

    def cmd_args(self):
        args = []
        for ifile in self.input_files:
            args.append(ifile.cmd_arg())
        return args

    def cmd_args_str(self):
        return ' '.join(self.cmd_args())

def parse_renamings(renaming_opts):
    renamings = defaultdict(dict)
    for rename in renaming_opts:
        left, right_name = rename.split('=')
        left_file, left_name = left.split('.')
        renamings[int(left_file) - 1][left_name] = right_name
    return renamings

def proper_reduce(func, args):
    args_iter = iter(args)
    res = list(islice(args_iter, 2))
    if len(res) == 1:
        return res[0]
    else:
        res = func(*list(res))
        for arg in args_iter:
            res = func(res, arg)
        return res

##
## OPTPARSE UTILS
##

def add_header(parser):
    parser.add_option(
        '-H', '--header', dest="header",
        help="assume there are no headers in input files and use HEADER instead",
    )

def add_no_out_header(parser):
    parser.add_option(
        '-N', '--no-out-header', dest="no_out_header", action="store_true",
        help="don't print resulting header (useful for mapreduce)",
    )

def add_pytrace(parser):
    parser.add_option(
        '--pytrace', dest="pytrace", action="store_true",
        help="verbose python errors"
    )

def add_print_cmd(parser):
    parser.add_option(
        '--print-cmd', dest="print_cmd", action="store_true",
        help="dry run, print commands that must be run",
    )

def add_awk_exec(parser):
    parser.add_option(
        '-A', '--awk-exec', dest="awk_exec",
        help="use AWK_EXEC as awk executable",
    )

class OptUtils(object):
    add_header = staticmethod(add_header)
    add_no_out_header = staticmethod(add_no_out_header)
    add_pytrace = staticmethod(add_pytrace)
    add_print_cmd = staticmethod(add_print_cmd)
    add_awk_exec = staticmethod(add_awk_exec)


### MODULE: tabkit.miniast

from _ast import *
from _ast import __version__

def parse(expr, filename='<unknown>', mode='exec'):
    """
    Parse an expression into an AST node.
    Equivalent to compile(expr, filename, mode, PyCF_ONLY_AST).
    """
    return compile(expr, filename, mode, PyCF_ONLY_AST)

def dump(node, annotate_fields=True, include_attributes=False):
    """
    Return a formatted dump of the tree in *node*.  This is mainly useful for
    debugging purposes.  The returned string will show the names and the values
    for fields.  This makes the code impossible to evaluate, so if evaluation is
    wanted *annotate_fields* must be set to False.  Attributes such as line
    numbers and column offsets are not dumped by default.  If this is wanted,
    *include_attributes* can be set to True.
    """
    def _format(node):
        if isinstance(node, AST):
            fields = [(a, _format(b)) for a, b in iter_fields(node)]
            rv = '%s(%s' % (node.__class__.__name__, ', '.join(
                ('%s=%s' % field for field in fields)
                if annotate_fields else
                (b for a, b in fields)
            ))
            if include_attributes and node._attributes:
                rv += fields and ', ' or ' '
                rv += ', '.join('%s=%s' % (a, _format(getattr(node, a)))
                                for a in node._attributes)
            return rv + ')'
        elif isinstance(node, list):
            return '[%s]' % ', '.join(_format(x) for x in node)
        return repr(node)
    if not isinstance(node, AST):
        raise TypeError('expected AST, got %r' % node.__class__.__name__)
    return _format(node)

def iter_fields(node):
    """
    Yield a tuple of ``(fieldname, value)`` for each field in ``node._fields``
    that is present on *node*.
    """
    if node._fields:
        for field in node._fields:
            try:
                yield field, getattr(node, field)
            except AttributeError:
                pass



### MODULE: tabkit.awk

# coding: utf-8

import _ast

pass # from tabkit.miniast import parse, dump
pass # from tabkit.datasrc import DataDesc, DataField, copy_field_order
pass # from tabkit.awk_expr import *
pass # from tabkit.awk_types import infer_type

OP_MAP = {
    _ast.Add      : lambda args: RowExprOp('+', args),
    _ast.Sub      : lambda args: RowExprOp('-', args),
    _ast.Mult     : lambda args: RowExprOp('*', args),
    _ast.Div      : lambda args: RowExprOp('/', args),
    _ast.Pow      : lambda args: RowExprOp('^', args),
    _ast.Mod      : lambda args: RowExprOp('%', args),
    _ast.FloorDiv : lambda args: RowExprFunc('int', [RowExprOp('/', args)]),

    _ast.And      : lambda args: RowExprOp('&&', args),
    _ast.Or       : lambda args: RowExprOp('||', args),
    _ast.Not      : lambda args: RowExprFunc('!', args),

    _ast.Eq      : lambda args: RowExprOp('==', args),
    _ast.NotEq   : lambda args: RowExprOp('!=', args),
    _ast.Gt      : lambda args: RowExprOp('>', args),
    _ast.Lt      : lambda args: RowExprOp('<', args),
    _ast.GtE     : lambda args: RowExprOp('>=', args),
    _ast.LtE     : lambda args: RowExprOp('<=', args),
}

def add_blocks(block1, block2):
    if isinstance(block1, AwkBlock):
        block1 = block1.lines
    elif isinstance(block1, AwkHeadBlock):
        block1 = [block1]
    if isinstance(block2, AwkBlock):
        block2 = block2.lines
    elif isinstance(block2, AwkHeadBlock):
        block2 = [block2]
    return AwkBlock(block1 + block2)

class AwkBlock(object):
    def __init__(self, lines=None):
        self.lines = lines or []
    def __nonzero__(self):
        return bool(self.lines)
    def append(self, line):
        self.lines.append(line)
    def __add__(self, block):
        return add_blocks(self, block)
    def __radd__(self, block):
        return add_blocks(block, self)
    def tostr(self, ident=0, newline='', level=0):
        strs = []
        for line in self.lines:
            if isinstance(line, AwkBlock):
                strs.append(
                    ' '*ident*level + '{' + newline
                    + line.tostr(ident, newline, level+1) + newline
                    + ' '*ident*level + '}'
                )
            elif isinstance(line, AwkHeadBlock):
                strs.append(
                    line.tostr(ident, newline, level)
                )
            else:
                if line:
                    line_str = ' '*ident*level + line
                    if not line_str.endswith(';'):
                        line_str += ';'
                    strs.append(line_str)
        return newline.join(strs)

class AwkHeadBlock(object):
    def __init__(self, header_str, block):
        self.header_str = header_str
        self.block = block
    def __add__(self, block):
        return add_blocks(self, block)
    def __radd__(self, block):
        return add_blocks(block, self)
    def tostr(self, ident=0, newline='', level=0):
        return (
            ' '*ident*level + self.header_str + newline
            + AwkBlock([self.block]).tostr(ident, newline, level)
        )

class AwkScript(object):
    """
    >>> awk = AwkScript(
    ...     begin = AwkBlock(['a=0']),
    ...     end = AwkBlock(['xx=0']),
    ...     main = AwkBlock([
    ...         'a=1',
    ...         AwkBlock(['b=2', 'c=3', 'd=4; e=5']),
    ...         'print',
    ...     ]),
    ... )
    >>> print awk.tostr(ident=0, newline='')
    BEGIN{a=0;}{a=1;{b=2;c=3;d=4; e=5;}print;}END{xx=0;}
    >>> print awk.tostr(ident=4, newline='\\n')
    BEGIN{
        a=0;
    }
    {
        a=1;
        {
            b=2;
            c=3;
            d=4; e=5;
        }
        print;
    }
    END{
        xx=0;
    }
    <BLANKLINE>
    """
    def __init__(self, main, begin=None, end=None, awk=None):
        self.awk = awk or 'awk'
        self.main = main
        self.begin = begin or AwkBlock([])
        self.end = end or AwkBlock([])
    def tostr(self, ident=0, newline=''):
        awk_str = ''
        if self.begin.lines:
            awk_str += 'BEGIN' + AwkBlock([self.begin]).tostr(ident, newline, 0) + newline
        awk_str += AwkBlock([self.main]).tostr(ident, newline, 0) + newline
        if self.end.lines:
            awk_str += 'END' + AwkBlock([self.end]).tostr(ident, newline, 0) + newline
        return awk_str
    def cmd_line(self, awk_exec=None, args=""):
        return "LC_ALL=C %s %s -F $'\\t' '%s'" % (awk_exec or self.awk, args, self.tostr(),)

def parse_expr(ctx, tree, subparser=None):
    subparser = subparser or parse_expr

    if isinstance(tree, _ast.Expr):
        return subparser(ctx, tree.value)
    elif isinstance(tree, _ast.Name):
        if ctx.has_field(tree.id):
            return RowExprField(ctx, tree.id)
        if tree.id in ctx.vars:
            return RowExprVar(ctx, tree.id)
        elif tree.id in ['NR', 'NF', 'FILENAME']:
            return RowExprBuiltinVar(tree.id)
        else:
            raise Exception('Variable %r not found' % (tree.id,))
    elif isinstance(tree, _ast.Call):
        assert not tree.keywords
        return RowExprFunc(
            tree.func.id, list(subparser(ctx, arg) for arg in tree.args)
        )
    elif isinstance(tree, _ast.Num):
        return RowExprConst(tree.n)
    elif isinstance(tree, _ast.Str):
        return RowExprConst(tree.s)
    elif isinstance(tree, _ast.BinOp):
        return OP_MAP[type(tree.op)]([subparser(ctx, tree.left), subparser(ctx, tree.right)])
    elif isinstance(tree, _ast.BoolOp):
        return OP_MAP[type(tree.op)]([subparser(ctx, val) for val in tree.values])
    elif isinstance(tree, _ast.UnaryOp):
        return OP_MAP[type(tree.op)]([subparser(ctx, tree.operand)])
    elif isinstance(tree, _ast.Compare):
        assert len(tree.ops) == 1
        if isinstance(tree.ops[0], _ast.In):
            assert len(tree.comparators) == 1
            args = tree.comparators[0].elts
            return RowExprOp(
                '||',
                [RowExprOp('==', [subparser(ctx, tree.left), subparser(ctx, arg)]) for arg in args],
            )
        elif isinstance(tree.ops[0], _ast.NotIn):
            assert len(tree.comparators) == 1
            args = tree.comparators[0].elts
            return RowExprOp(
                '&&',
                [RowExprOp('!=', [subparser(ctx, tree.left), subparser(ctx, arg)]) for arg in args],
            )
        else:
            return OP_MAP[type(tree.ops[0])](
                [subparser(ctx, val) for val in [tree.left] + tree.comparators]
            )
    elif isinstance(tree, _ast.IfExp):
        return RowExprIf(
            subparser(ctx, tree.test),
            subparser(ctx, tree.body),
            subparser(ctx, tree.orelse),
        )
    elif isinstance(tree, _ast.Assign):
        raise Exception('Invalid assignment in %r' % (dump(tree),))
    else:
        raise Exception('Unrecognized node %r' % (tree,))

def parse_assign_expr(ctx, tree, subparser):
    if isinstance(tree, _ast.Assign):
        assert len(tree.targets) == 1 and isinstance(tree.targets[0], _ast.Name)
        return RowExprAssign(tree.targets[0].id, subparser(ctx, tree.value))
    elif isinstance(tree, _ast.Expr):
        if isinstance(tree.value, _ast.Name):
            return RowExprAssign(tree.value.id, subparser(ctx, tree))
        else:
            raise Exception("Please assign expression to a variable")
    else:
        raise Exception("Please assign expression to a variable")

def parse_rowexpr(ctx, tree):
    return parse_assign_expr(ctx, tree, parse_expr)

def find_all_refered_vars(ctx):
    refered_vars = set()
    for val in ctx.vars.itervalues():
        for node in val.find(RowExprVar, {}):
            refered_vars.add(node.name)
    return refered_vars

def find_refered_vars(ctx, expr):
    for node in expr.find(RowExprVar, {}):
        yield node

def awk_filter_map(data_desc, filter_str, map_strs):
    """
    >>> from tabkit.header import parse_header
    >>> awk, desc = awk_filter_map(
    ...     parse_header('# d p e s c m'),
    ...     'e==157 and (s>100 or s in [15,30,45])',
    ...     ['ctr=c/s', 'cpm=ctr*m']
    ... )
    >>> print desc
    DataDesc([DataField('ctr', 'any'), DataField('cpm', 'any')])
    >>> print awk.cmd_line()
    LC_ALL=C awk  -F $'\\t' 'BEGIN{OFS="\\t";}{if((($3 == 157) && (($4 > 100) || (($4 == 15) || ($4 == 30) || ($4 == 45))))){ctr = ($5 / $4);print(ctr,(ctr * $6));}}'
    """
    ctx = ExprContext(data_desc)

    # parse map
    for map_expr_str in map_strs:
        for node in parse(map_expr_str).body:
            expr = parse_rowexpr(ctx, node)
            ctx.set_var(expr.target, expr.value)

    # parse filter
    nodes = parse(filter_str).body
    filter_expr = None
    if len(nodes) == 0:
        pass
    elif len(nodes) == 1:
        filter_expr = parse_expr(ctx, nodes[0])
    else:
        raise Exception('Multiple expressions in filter are not allowed')

    awk_cmd, output_desc = awk_filter_map_from_context(ctx, filter_expr, data_desc.order)
    return awk_cmd, output_desc or data_desc

def awk_filter_map_from_context(ctx, filter_expr=None, order=None):
    order = order or []
    refered_by_filter = set()
    if filter_expr != None:
        refered_by_filter = set(node.name for node in find_refered_vars(ctx, filter_expr))
    refered_vars = find_all_refered_vars(ctx)
    assign_before_if = []
    assign_after_if = []
    statements = []
    varnames = []
    kept_fields = {}
    for name, val in ctx.itervars():
        if not name.startswith('_'):
            varnames.append(name)
            if isinstance(val, RowExprField):
                kept_fields[val.name] = name
            elif (
                isinstance(val, RowExprVar)
                and hasattr(val, 'ctx') # ctx отсутствует у _GrpExprFunc
            ):
                if isinstance(val.ctx.vars[val.name], RowExprField):
                    kept_fields[val.ctx.vars[val.name].name] = name
        if name in refered_by_filter:
            assign_before_if.append(RowExprAssign(name, val))
            if not name.startswith('_'):
                statements.append(RowExprVar(ctx, name))
        elif name in refered_vars:
            if not name.startswith('__'):
                assign_after_if.append(RowExprAssign(name, val))
            if not name.startswith('_'):
                statements.append(RowExprVar(ctx, name))
        elif not name.startswith('_'):
            statements.append(val)

    if statements:
        awk_cmd = AwkBlock(["print(" + ','.join(expr.tostr() for expr in statements) + ")"])
    else:
        awk_cmd = AwkBlock(["print"])
    if assign_after_if:
        awk_cmd = AwkBlock(['; '.join(expr.tostr() for expr in assign_after_if)]) + awk_cmd
    if filter_expr:
        awk_cmd = AwkBlock([AwkHeadBlock('if(%s)' % (filter_expr.tostr(),), awk_cmd)])
    if assign_before_if:
        awk_cmd = AwkBlock(['; '.join(expr.tostr() for expr in assign_before_if)]) + awk_cmd
    awk_cmd = AwkScript(awk_cmd, begin=AwkBlock(['OFS="\\t"']))

    # construct data_desc
    types = infer_type(statements)
    output_fields = list(DataField(name, type) for name, type in zip(varnames, types))
    new_order = []
    for field in order:
        if field.name not in kept_fields:
            break
        new_order.append(copy_field_order(
            field,
            name = kept_fields[field.name],
        ))

    if output_fields:
        return awk_cmd, DataDesc(output_fields, new_order)
    else:
        return awk_cmd, None

def _test():
    import doctest
    doctest.testmod()

# if __name__ == "__main__":
#     _test()
# 

### MODULE: tabkit.awk_grp

import _ast

pass # from tabkit.miniast import parse
pass # from tabkit.utils import partial
pass # from tabkit.datasrc import DataDesc

pass # from tabkit.awk import parse_expr, parse_assign_expr
pass # from tabkit.awk import awk_filter_map_from_context, match_node
pass # from tabkit.awk import ExprContext, RowExprOp, RowExprVar, RowExprConst
pass # from tabkit.awk import AwkBlock, AwkScript, AwkHeadBlock
pass # from tabkit.awk_expr import _GrpExprFunc

class Namer(object):
    def __init__(self, prefix):
        self.prefix = prefix
        self.cnt = 0
        self.names = {}
    def get_name(self, obj):
        if obj in self.names:
            return self.names[obj]
        else:
            name = self.prefix + str(self.cnt)
            self.names[name] = obj
            self.cnt += 1
            return name

class GrpExprFuncMaker(object):
    def __init__(self, prefix):
        self.namer = Namer(prefix)
    def __call__(self, func, init, update, args, end=None):
        return _GrpExprFunc(
            name = self.namer.get_name((init, update) + tuple(arg.tostr() for arg in args)),
            func = func,
            init = init,
            update = update,
            args = args,
            end = end,
        )

def grp_ifmax(maker, args):
    if len(args) != 2:
        raise Exception("'ifmax' function takes 2 arguments")
    return maker(
        func = "ifmax",
        init = '%(var)s_cmp = -10^1000000; %(var)s_cmp = -10^1000000;',
        update = (
            '__tmp__=%(rowexpr0)s; '
            'if(__tmp__>%(var)s_cmp)'
            '{%(var)s_cmp=__tmp__; %(var)s=%(rowexpr1)s};'
        ),
        args = args,
    )

def grp_ifmin(maker, args):
    if len(args) != 2:
        raise Exception("'ifmin' function takes 2 arguments")
    return maker(
        func = "ifmin",
        init = '%(var)s_cmp = 10^1000000; %(var)s_cmp = 10^1000000;',
        update = (
            '__tmp__=%(rowexpr0)s; '
            'if(__tmp__<%(var)s_cmp)'
            '{%(var)s_cmp=__tmp__; %(var)s=%(rowexpr1)s};'
        ),
        args = args,
    )

def grp_max(maker, args):
    if len(args) != 1:
        raise Exception("'max' function takes 1 argument")
    return maker(
        func = "max",
        init = '%(var)s = -10^1000000;',
        update = '__tmp__=%(rowexpr0)s; if(__tmp__>%(var)s){%(var)s=__tmp__};',
        args = args,
    )

def grp_min(maker, args):
    if len(args) != 1:
        raise Exception("'min' function takes 1 argument")
    return maker(
        func = "min",
        init = '%(var)s = 10^1000000;',
        update = '__tmp__=%(rowexpr0)s; if(__tmp__<%(var)s){%(var)s=__tmp__};',
        args = args,
    )

def grp_sum(maker, args):
    if len(args) != 1:
        raise Exception("'sum' function takes 1 argument")
    return maker(func="sum", init='%(var)s = 0;', update='%(var)s += %(rowexpr0)s;', args=args)

def grp_concat(maker, args):
    if len(args) == 1:
        field_name, = args
        delim = RowExprConst(",")
    elif len(args) == 2:
        field_name, delim = args
    else:
        raise Exception("'concat' function takes 2 arguments (field and delimiter")

    if not isinstance(delim, RowExprConst) or delim.type != "str":
        raise Exception("'delim' arg to 'concat' function should be a const of type 'str'")

    return maker(
        func = "concat",
        init = '%(var)s = "";',
        update = '%(var)s = (%(var)s=="")?(%(rowexpr0)s):(%(var)s"' + delim.const + '"%(rowexpr0)s);',
        args = (field_name,),
    )

def grp_cnt(maker, args):
    if len(args) != 0:
        raise Exception("'cnt' function takes no arguments")
    return maker(func="cnt", init='%(var)s = 0;', update='%(var)s += 1;', args=args)

def grp_avg(maker, args):
    if len(args) != 1:
        raise Exception("'avg' function takes 1 argument")
    return RowExprOp('/', [grp_sum(maker, args), grp_cnt(maker, [])])
    
def grp_median(maker, args):
    if len(args) != 1:
        raise Exception("'median' function takes 1 argument")
    return maker(
        func = "median",
        init = "delete %(var)s_arr; %(var)s_cnt = 0;",
        update = "%(var)s_arr[%(var)s_cnt] = %(rowexpr0)s; %(var)s_cnt++;",
        args = args,
        end = "__tmp__ = asort(%(var)s_arr) / 2; %(var)s = int(__tmp__) == __tmp__ ? (%(var)s_arr[int(__tmp__)] + %(var)s_arr[int(__tmp__) + 1]) / 2 : %(var)s_arr[int(__tmp__) + 1]"
    )
    
def grp_variance(maker, args):
    if len(args) != 1:
        raise Exception("'variance' function takes 1 argument")
    return RowExprOp('-', 
        [
            RowExprOp('/', [grp_sum(maker, [RowExprOp('^', [args[0], RowExprConst(2)])]), grp_cnt(maker, [])]),
            RowExprOp('^', [
                RowExprOp('/', [grp_sum(maker, args), grp_cnt(maker, [])]),
                RowExprConst(2)
            ])
        ]
    )

FUNC_MAP = {
    'ifmin' : grp_ifmin,
    'ifmax' : grp_ifmax,
    'min' : grp_min,
    'max' : grp_max,
    'sum' : grp_sum,
    'concat' : grp_concat,
    'cnt' : grp_cnt,
    'avg' : grp_avg,
    'median' : grp_median,
    'var' : grp_variance
}

def parse_grpexpr(grp_ctx, tree, row_ctx, maker):
    if isinstance(tree, _ast.Call) and tree.func.id in FUNC_MAP:
        assert not tree.keywords
        return FUNC_MAP[tree.func.id](
            maker, list(parse_expr(row_ctx, arg) for arg in tree.args)
        )
    else:
        return parse_expr(grp_ctx, tree, partial(parse_grpexpr, maker=maker, row_ctx=row_ctx))

def parse_assign_grpexpr(grp_ctx, tree, row_ctx, maker):
    return parse_assign_expr(grp_ctx, tree, partial(parse_grpexpr, maker=maker, row_ctx=row_ctx))

def find_grp_funcs(ctx):
    func_dict = {}
    for name, val in ctx.itervars():
        for node in val.find(_GrpExprFunc, {}):
            func_dict[node.name] = node
    return func_dict.items()

def awk_grp(data_desc, key_str, grp_expr_tuples):
    acc_maker = GrpExprFuncMaker('__acc_')
    grp_maker = GrpExprFuncMaker('__grp_')
    key_ctx = ExprContext(data_desc, do_substitute_vars=True)
    row_ctx = ExprContext(data_desc)
    acc_ctx = ExprContext(DataDesc([],[]))
    grp_ctx = ExprContext(DataDesc([],[]))
    out_ctx = ExprContext(DataDesc([],[]))
    for grp_type, expr_str in grp_expr_tuples:
        for ast_expr in parse(expr_str).body:
            if grp_type == 'acc':
                expr = parse_assign_grpexpr(acc_ctx, ast_expr, row_ctx, acc_maker)
                key_ctx.set_var(expr.target, expr.value)
                acc_ctx.set_var(expr.target, expr.value)
                out_ctx.set_var(expr.target, expr.value)
            elif grp_type == 'grp':
                expr = parse_assign_grpexpr(grp_ctx, ast_expr, row_ctx, grp_maker)
                grp_ctx.set_var(expr.target, expr.value)
                out_ctx.set_var(expr.target, expr.value)
            else:
                raise Exception('Unknown grouping type %r' % (grp_type,))

    # parse key expr
    key_namer = Namer('__key')
    key_row_namer = Namer('__row_key')
    keys = []
    key_ins_pos = 0
    for node in parse(key_str or '1').body:
        if isinstance(node, _ast.Assign):
            assert len(node.targets) == 1 and isinstance(node.targets[0], _ast.Name)
            expr = parse_expr(key_ctx, node.value)
            key_name = key_namer.get_name(expr)
            key_row_name = key_row_namer.get_name(expr)
            out_ctx.set_var(key_name, expr)
            out_ctx.set_var(node.targets[0].id, RowExprVar(out_ctx, key_name), insert_at=key_ins_pos)
            key_ins_pos += 1
        else:
            expr = parse_expr(key_ctx, node)
            key_name = key_namer.get_name(expr)
            key_row_name = key_row_namer.get_name(expr)
        keys.append((expr, key_name, key_row_name))

    # construct awk script
    print_awk, output_desc = awk_filter_map_from_context(out_ctx, order=data_desc.order)
    assert not print_awk.end

    init_grps = AwkBlock()
    init_accs = AwkBlock()
    calc_row_keys = AwkBlock()
    keys_changed = []
    update_keys = AwkBlock()
    update_grps = AwkBlock()
    update_accs = AwkBlock()
    end_grps = AwkBlock()

    for expr, name, row_name in keys:
        calc_row_keys.append(row_name + ' = ' + expr.tostr())
        update_keys.append(name + ' = ' + row_name)
        keys_changed.append(name + '!=' + row_name)

    for name, val in find_grp_funcs(grp_ctx):
        init_grps.append(val.init_str())
        update_grps.append(val.update_str())
        end_grps.append(val.end_str())

    for name, val in find_grp_funcs(acc_ctx):
        init_accs.append(val.init_str())
        update_accs.append(val.update_str())

    keys_changed_str = ' || '.join(keys_changed)

    awk = AwkScript(
        begin = (
            print_awk.begin
            + init_grps
            + init_accs
            + AwkBlock(['__print_last = ' + str(int(key_str == None))])
        ),
        end = AwkBlock([
            AwkHeadBlock('if(NR!=0 || __print_last==1)', end_grps + print_awk.main)
        ]),
        main = (
            calc_row_keys
            + AwkHeadBlock('if(NR==1)', update_keys)
            + AwkHeadBlock('else', AwkBlock([
                AwkHeadBlock('if(' + keys_changed_str + ')',
                    end_grps
                    + print_awk.main
                    + update_keys
                    + init_grps
                )])
            )
            + update_grps
            + update_accs
        )
    )

    return awk, output_desc or data_desc

__test__ = dict(
    awk_grp1 = r"""
        >>> from tabkit.header import parse_header
        >>> awk_cmd, output_desc = awk_grp(
        ...     data_desc = parse_header('# d p e s c m'),
        ...     key_str = 'd;p',
        ...     grp_expr_tuples = [
        ...         ('grp', 'ctr=sum(c)/sum(s); cpm=ctr*avg(m)'),
        ...         ('acc', 'cnt=cnt()'),
        ...         ('grp', 'xctr=avg(c/s)'),
        ...     ],
        ... )
        >>> print awk_cmd.tostr(ident=4, newline='\n')
        BEGIN{
            OFS="\t";
            __grp_5 = 0;
            __grp_4 = 0;
            __grp_1 = 0;
            __grp_0 = 0;
            __grp_3 = 0;
            __grp_2 = 0;
            __acc_0 = 0;
            __print_last = 0;
        }
        {
            __row_key0 = $1;
            __row_key1 = $2;
            if(NR==1)
            {
                __key0 = __row_key0;
                __key1 = __row_key1;
            }
            else
            {
                if(__key0!=__row_key0 || __key1!=__row_key1)
                {
                    ctr = (__grp_0 / __grp_1);
                    print(ctr,(ctr * (__grp_2 / __grp_3)),__acc_0,(__grp_4 / __grp_5));
                    __key0 = __row_key0;
                    __key1 = __row_key1;
                    __grp_5 = 0;
                    __grp_4 = 0;
                    __grp_1 = 0;
                    __grp_0 = 0;
                    __grp_3 = 0;
                    __grp_2 = 0;
                }
            }
            __grp_5 += 1;
            __grp_4 += ($5 / $4);
            __grp_1 += $4;
            __grp_0 += $5;
            __grp_3 += 1;
            __grp_2 += $6;
            __acc_0 += 1;
        }
        END{
            if(NR!=0 || __print_last==1)
            {
                ctr = (__grp_0 / __grp_1);
                print(ctr,(ctr * (__grp_2 / __grp_3)),__acc_0,(__grp_4 / __grp_5));
            }
        }
        <BLANKLINE>
        """,
    awk_grp2 = r"""
        >>> from tabkit.header import parse_header
        >>> awk_cmd, output_desc = awk_grp(
        ...     data_desc = parse_header('# d p e s c m'),
        ...     key_str = 'grp=int(_ctr*100)',
        ...     grp_expr_tuples = [
        ...         ('acc', '_ctr=sum(c)/(sum(s)+0.0000001)*100'),
        ...         ('acc', 'm=sprintf("%0.2f",sum(m)/1000000)'),
        ...         ('acc', 'cpm=sprintf("%0.20f",_ctr*m)'),
        ...         ('grp', 'cnt=cnt()'),
        ...     ],
        ... )
        >>> print awk_cmd.tostr(ident=4, newline='\n')
        BEGIN{
            OFS="\t";
            __grp_0 = 0;
            __acc_2 = 0;
            __acc_1 = 0;
            __acc_0 = 0;
            __print_last = 0;
        }
        {
            __row_key0 = int((((__acc_0 / (__acc_1 + 1e-07)) * 100) * 100));
            if(NR==1)
            {
                __key0 = __row_key0;
            }
            else
            {
                if(__key0!=__row_key0)
                {
                    _ctr = ((__acc_0 / (__acc_1 + 1e-07)) * 100); m = sprintf("%0.2f", (__acc_2 / 1000000));
                    print(__key0,m,sprintf("%0.20f", (_ctr * m)),__grp_0);
                    __key0 = __row_key0;
                    __grp_0 = 0;
                }
            }
            __grp_0 += 1;
            __acc_2 += $6;
            __acc_1 += $4;
            __acc_0 += $5;
        }
        END{
            if(NR!=0 || __print_last==1)
            {
                _ctr = ((__acc_0 / (__acc_1 + 1e-07)) * 100); m = sprintf("%0.2f", (__acc_2 / 1000000));
                print(__key0,m,sprintf("%0.20f", (_ctr * m)),__grp_0);
            }
        }
        <BLANKLINE>
        """,
    awk_grp3 = r"""
        >>> from tabkit.header import parse_header
        >>> awk_cmd, output_desc = awk_grp(
        ...     data_desc = parse_header('# d p e s c m'),
        ...     key_str = 'd;p',
        ...     grp_expr_tuples = [
        ...         ('grp', 'median_ctr=median(c/s)'),
        ...         ('grp', 'variance_ctr=var(c/s)'),
        ...     ],
        ... )
        >>> print awk_cmd.tostr(ident=4, newline='\n')
        BEGIN{
            OFS="\t";
            __grp_4 = 0;
            __grp_1 = 0;
            delete __grp_0_arr; __grp_0_cnt = 0;
            __grp_3 = 0;
            __grp_2 = 0;
            __print_last = 0;
        }
        {
            __row_key0 = $1;
            __row_key1 = $2;
            if(NR==1)
            {
                __key0 = __row_key0;
                __key1 = __row_key1;
            }
            else
            {
                if(__key0!=__row_key0 || __key1!=__row_key1)
                {
                    __tmp__ = asort(__grp_0_arr) / 2; __grp_0 = int(__tmp__) == __tmp__ ? (__grp_0_arr[int(__tmp__)] + __grp_0_arr[int(__tmp__) + 1]) / 2 : __grp_0_arr[int(__tmp__) + 1];
                    print(__grp_0,((__grp_1 / __grp_2) - ((__grp_3 / __grp_4) ^ 2)));
                    __key0 = __row_key0;
                    __key1 = __row_key1;
                    __grp_4 = 0;
                    __grp_1 = 0;
                    delete __grp_0_arr; __grp_0_cnt = 0;
                    __grp_3 = 0;
                    __grp_2 = 0;
                }
            }
            __grp_4 += 1;
            __grp_1 += (($5 / $4) ^ 2);
            __grp_0_arr[__grp_0_cnt] = ($5 / $4); __grp_0_cnt++;
            __grp_3 += ($5 / $4);
            __grp_2 += 1;
        }
        END{
            if(NR!=0 || __print_last==1)
            {
                __tmp__ = asort(__grp_0_arr) / 2; __grp_0 = int(__tmp__) == __tmp__ ? (__grp_0_arr[int(__tmp__)] + __grp_0_arr[int(__tmp__) + 1]) / 2 : __grp_0_arr[int(__tmp__) + 1];
                print(__grp_0,((__grp_1 / __grp_2) - ((__grp_3 / __grp_4) ^ 2)));
            }
        }
        <BLANKLINE>
        """
)

def _test():
    import doctest
    doctest.testmod()

# if __name__ == "__main__":
#     _test()

### MODULE: tabkit.awk_expr

class RowExprAssign(object):
    def __init__(self, target, value):
        self.target = target
        self.value = value
    def tostr(self):
        return self.target + ' = ' + self.value.tostr()

class RowExprConst(object):
    def __init__(self, const):
        self.const = const
        if isinstance(const, str):
            self.type = "str"
        elif isinstance(const, int):
            self.type = "int"
        elif isinstance(const, float):
            self.type = "float"
        elif isinstance(const, bool):
            self.type = "bool"
        else:
            raise Exception("Unsupported type of const %r" % (const,))
    def tostr(self):
        if isinstance(self.const, str):
            return '"' + self.const.replace('"', r'\"') + '"'
        elif isinstance(self.const, bool):
            return str(int(self.const))
        else:
            return str(self.const)
    def find(self, node_type, node_props):
        return match_node(self, node_type, node_props)

class RowExprField(object):
    def __init__(self, ctx, name):
        self.ctx = ctx
        self.name = name
        if self.name.startswith('__'):
            raise Exception("Variable name can't start with '__'")
    def tostr(self):
        return '$' + str(self.ctx.data_desc.field_index(self.name) + 1)
    def find(self, node_type, node_props):
        return match_node(self, node_type, node_props)

class RowExprBuiltinVar(object):
    def __init__(self, name):
        self.name = name
    def tostr(self):
        return self.name
    def find(self, node_type, node_props):
        return []

class RowExprVar(object):
    def __init__(self, ctx, name):
        self.ctx = ctx
        self.name = name
    def tostr(self):
        if self.ctx.do_substitute_var(self.name):
            return self.ctx.vars[self.name].tostr()
        else:
            return self.name
    def get_var_expr(self):
        return self.ctx.vars.get(self.name)
    def find(self, node_type, node_props):
        for node in match_node(self, node_type, node_props):
            yield node
        var_expr = self.get_var_expr()
        if var_expr:
            for node in var_expr.find(node_type, node_props):
                yield node

class RowExprFunc(object):
    def __init__(self, func, args):
        self.func = func
        self.args = args
    def tostr(self):
        return self.func + '(' + ', '.join(arg.tostr() for arg in self.args) + ')'
    def find(self, node_type, node_props):
        for node in match_node(self, node_type, node_props):
            yield node
        for arg in self.args:
            for node in arg.find(node_type, node_props):
                yield node

class RowExprOp(object):
    def __init__(self, op, args):
        self.op = op
        self.args = args
    def tostr(self):
        return '(' + (' ' + self.op + ' ').join(arg.tostr() for arg in self.args) + ')'
    def find(self, node_type, node_props):
        for node in match_node(self, node_type, node_props):
            yield node
        for arg in self.args:
            for node in arg.find(node_type, node_props):
                yield node

class RowExprIf(object):
    def __init__(self, test, body, orelse):
        self.test = test
        self.body = body
        self.orelse = orelse
    def tostr(self):
        return "(%s?%s:%s)" % (
            self.test.tostr(),
            self.body.tostr(),
            self.orelse.tostr(),
        )
    def find(self, node_type, node_props):
        for node in match_node(self, node_type, node_props):
            yield node
        for arg in [self.test, self.body, self.orelse]:
            for node in arg.find(node_type, node_props):
                yield node

class ExprContext(object):
    def __init__(self, data_desc, do_substitute_vars=False):
        self.data_desc = data_desc
        self.vars = {}
        self.varnames = []
        self.do_substitute_vars = do_substitute_vars
    def has_field(self, name):
        return self.data_desc.has_field(name)
    def do_substitute_var(self, name):
        return self.do_substitute_vars
    def itervars(self):
        for name in self.varnames:
            yield name, self.vars[name]
    def set_var(self, name, val, insert_at=None):
        if name not in self.vars:
            self.vars[name] = val
            if insert_at == None:
                self.varnames.append(name)
            else:
                self.varnames.insert(insert_at, name)
        else:
            raise Exception("Variable named %r already exists" % (name,))

class _GrpExprFunc(RowExprVar):
    def __init__(self, name, func, init, update, args, end=None):
        self.name = name
        self.func = func
        self.init = init
        self.update = update
        self.args = args
        if end:
            self.end = end
    def tostr(self):
        return self.name
    def find(self, node_type, node_props):
        return match_node(self, node_type, node_props)
    def _expand_tpl(self, tpl):
        args = dict(var=self.name)
        for num, arg in enumerate(self.args):
            args['rowexpr%s' % (num,)] = arg.tostr()
        return tpl % args
    def init_str(self):
        return self._expand_tpl(self.init)
    def update_str(self):
        return self._expand_tpl(self.update)
    def end_str(self):
        if hasattr(self, 'end'):
            return self._expand_tpl(self.end)
        else:
            return ''

def match_node(node, node_type, node_props):
    if not isinstance(node, node_type):
        return iter([])
    for name, val in node_props.iteritems():
        if not (hasattr(node, name) and getattr(node, name) == val):
            return iter([])
    return iter([node])            
### MODULE: tabkit.awk_types

pass # from tabkit.awk_expr import *
pass # from tabkit.awk_expr import _GrpExprFunc

def infer_type(obj):
    if isinstance(obj, list):
        return (infer_type(expr) for expr in obj)
    if isinstance(obj, RowExprConst):
        return obj.type
    if isinstance(obj, RowExprField):
        return obj.ctx.data_desc.get_field(obj.name).type
    if isinstance(obj, RowExprVar):
        if isinstance(obj, _GrpExprFunc):
            if obj.func in ["ifmin", "ifmax"]:
                return infer_type(obj.args[1])
            if obj.func in ["max", "min", "sum", "median"]:
                type = infer_type(obj.args[0])
                if type not in ['int', 'float']:
                    type = 'float'
                return type
            if obj.func == "cnt":
                return "int"
            if obj.func == "concat":
                return "str"
        else:
            return infer_type(obj.get_var_expr())
    if isinstance(obj, RowExprOp):
        arg_types = list(infer_type(arg) for arg in obj.args)
        if obj.op in ["&&", "||", "!", "==", "!=", ">", "<", ">=", "<="]:
            return "bool"
        if obj.op in "+-*^%":
            if "any" in arg_types:
                return "any"
            if "float" in arg_types:
                return "float"
            return "int"
        if obj.op == "/":
            if "any" in arg_types:
                return "any"
            return "float"
        print obj.op, arg_types
    if isinstance(obj, RowExprFunc):
        if obj.func == "int":
            return "int"
        if obj.func == "sprintf":
            return "str"
        if obj.func == "log":
            return "float"            
    return "any"


### MAIN


import sys
import os
from optparse import OptionParser, Option
from itertools import islice, groupby

pass # from tabkit.datasrc import DataDesc, DataOrder, convertible
pass # from tabkit.header import make_header
pass # from tabkit.utils import safe_system, exception_handler, FilesList, OptUtils

def main():
    optparser = OptionParser(
        usage = '%prog [options] [files]',
        option_list = [
            Option('-f', dest="fields_keep", action="append", help="fields to keep", default=[]),
            Option('-r', dest="fields_remove", action="append", help="or fields to remove", default=[]),
            Option('-z', dest="gzip", action="store_true", help="assume all plain files are gzipped"),
        ],
    )
    OptUtils.add_pytrace(optparser)
    OptUtils.add_print_cmd(optparser)
    opts, args = optparser.parse_args()

    files = FilesList(args, gzip=opts.gzip)

    # calc fields and their types
    first_desc = list(islice(files, 1))[0].desc()
    fields = []
    if opts.fields_keep and opts.fields_remove:
        raise Exception('-f and -r options are exclusive')
    if opts.fields_keep:
        first_fields_map = dict((field.name, field) for field in first_desc.fields)
        for field in opts.fields_keep:
            for subfield in field.split(','):
                fields.append(first_fields_map[subfield])
    if opts.fields_remove:
        field_names = []
        for field in opts.fields_remove:
            field_names.extend(field.split(','))
        for field in first_desc.fields:
            if not field.name in field_names:
                fields.append(field)
    if not fields:
        raise Exception(
            'Specify at least one output field name with -f'
            ' option or at least one field to remove with -r option'
        )
    field_names = list(field.name for field in fields)

    # calc ordering
    order = None
    if len(files) == 1:
        for fname, desc in files.names_descs():
            order_fields = []
            for field in desc.order:
                if field.name in field_names:
                    order_fields.append(field)
                else:
                    break
            order = DataOrder(order_fields)

    out_desc = DataDesc(fields, order=order, size=files.get_size())

    if not opts.print_cmd:
        os.write(sys.stdout.fileno(), make_header(out_desc))

    for grp_fields, ifiles in groupby(files, lambda ifile: ifile.desc().fields):
        grp_idx_map = dict((field.name, fnum + 1) for fnum, field in enumerate(grp_fields))
        grp_fields_map = dict((field.name, field) for fnum, field in enumerate(grp_fields))
        cut_fields = []
        for field in fields:
            if field.name not in grp_idx_map:
                raise Exception('Field %r not found in input file %r' % (field.name, fname))
            if cut_fields and grp_idx_map[field.name] <= max(cut_fields):
                raise Exception('Incompatible position of field %r in %r' % (field.name, fname))
            if not convertible(grp_fields_map[field.name].type, field.type):
                raise Exception('Incompatible type %r (was %r) of field %r in file %r' % (
                    grp_fields_map[field.name].type,
                    field.type,
                    field.name,
                    fname
                ))
            cut_fields.append(grp_idx_map[field.name])

        cut_fields = ','.join(map(str, cut_fields))
        cmd = 'cut -f %s %s' % (cut_fields, ' '.join(ifile.cmd_arg() for ifile in ifiles))
        if opts.print_cmd:
            print cmd
        else:
            safe_system(cmd)

if __name__ == '__main__':
    exception_handler(main)

